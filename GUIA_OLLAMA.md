# üöÄ Gu√≠a: Usar Ollama con GPU (Alternativa a llama-cpp-python)

## ¬øPor qu√© Ollama?

‚úÖ **Detecta GPU autom√°ticamente** - No necesitas configurar nada
‚úÖ **M√°s f√°cil de instalar** - Un solo comando
‚úÖ **Mejor rendimiento** - Optimizado para inferencia
‚úÖ **Gesti√≥n de modelos simple** - Pull/delete/list como Docker
‚úÖ **Compatible con tu hardware** - Funciona con NVIDIA, AMD, CPU

## Instalaci√≥n

### 1. Descargar Ollama

```powershell
# Opci√≥n A: Desde el navegador
# Ir a: https://ollama.com/download
# Descargar el instalador para Windows

# Opci√≥n B: Con winget (si lo tienes)
winget install Ollama.Ollama
```

### 2. Verificar instalaci√≥n

```powershell
ollama --version
```

### 3. Descargar modelos

```powershell
# Modelo recomendado para tu proyecto (3GB)
ollama pull llama3.2:3b

# Alternativas:
ollama pull llama3.2:1b     # M√°s r√°pido, menos preciso (1.3GB)
ollama pull llama3.1:8b     # M√°s preciso, m√°s lento (4.7GB)
ollama pull qwen2.5:3b      # Alternativa a Llama (2GB)
```

### 4. Verificar GPU

```powershell
ollama ps
# Deber√≠a mostrar que est√° usando la GPU autom√°ticamente
```

## Uso en tu proyecto

### Opci√≥n A: Script standalone

```powershell
python generador_examenes_ollama.py
```

### Opci√≥n B: Integrar en tu c√≥digo existente

```python
from generador_examenes_ollama import GeneradorExamenesOllama

# Crear generador (detecta GPU autom√°ticamente)
generador = GeneradorExamenesOllama(modelo="llama3.2:3b")

# Generar examen
contenido = "Tu contenido aqu√≠..."
preguntas = generador.generar_examen(
    contenido, 
    num_preguntas={'multiple': 5, 'verdadero_falso': 3}
)
```

### Opci√≥n C: Modificar tu c√≥digo actual

Reemplaza en tus scripts:

```python
# ANTES (llama-cpp-python)
from llama_cpp import Llama
llm = Llama(model_path="modelos/modelo.gguf", n_gpu_layers=35)

# DESPU√âS (Ollama)
from generador_examenes_ollama import GeneradorExamenesOllama
generador = GeneradorExamenesOllama(modelo="llama3.2:3b")
```

## Comandos √∫tiles

```powershell
# Ver modelos instalados
ollama list

# Ver modelos corriendo (y uso de GPU)
ollama ps

# Eliminar modelo
ollama rm llama3.2:3b

# Probar modelo en consola
ollama run llama3.2:3b
```

## Ventajas sobre llama-cpp-python

| Caracter√≠stica | llama-cpp-python | Ollama |
|----------------|------------------|---------|
| Instalaci√≥n GPU | Compleja (CMAKE, CUDA toolkit) | Autom√°tica |
| Gesti√≥n modelos | Manual (archivos .gguf) | `ollama pull` |
| Detecci√≥n GPU | Manual (`n_gpu_layers`) | Autom√°tica |
| Rendimiento | Bueno | Excelente |
| Facilidad de uso | Media | Alta |

## Comparaci√≥n de modelos

Para tu proyecto de ex√°menes, recomiendo:

1. **llama3.2:3b** (3GB) - Balance perfecto calidad/velocidad
2. **qwen2.5:3b** (2GB) - Alternativa con buen espa√±ol
3. **llama3.1:8b** (4.7GB) - Si necesitas m√°xima calidad

## Verificar que usa GPU

Cuando ejecutes el modelo, ver√°s en la terminal:

```
‚úÖ Ollama activo - 1 modelos disponibles
ü§ñ Generando 10 preguntas con llama3.2:3b...
```

Y en otra terminal:
```powershell
ollama ps
# Salida:
NAME              ID           SIZE    PROCESSOR    UNTIL
llama3.2:3b       961cd76...   3.0GB   100% GPU     4 minutes from now
```

Si dice **"100% GPU"** = ¬°Est√° usando tu tarjeta gr√°fica! üéâ

## Soluci√≥n de problemas

### Ollama no usa GPU

```powershell
# Verificar drivers NVIDIA
nvidia-smi

# Reiniciar servicio Ollama
ollama serve
```

### Modelo no encontrado

```powershell
ollama list  # Ver modelos instalados
ollama pull llama3.2:3b  # Descargar si falta
```

## Migraci√≥n completa

Si quieres migrar todo tu proyecto a Ollama:

1. Instalar Ollama
2. Descargar modelos: `ollama pull llama3.2:3b`
3. Reemplazar imports en tu c√≥digo
4. Opcional: desinstalar `pip uninstall llama-cpp-python`

## Recursos

- Documentaci√≥n: https://github.com/ollama/ollama
- Modelos disponibles: https://ollama.com/library
- Discord: https://discord.gg/ollama
