# ğŸ® Estado del Soporte GPU

## âŒ Problema Actual

Tu sistema tiene:
- âœ… GPU: NVIDIA GeForce RTX 4050 (6GB VRAM)
- âœ… CUDA 12.6 instalado
- âŒ llama-cpp-python **NO** tiene soporte GPU habilitado

## ğŸ” DiagnÃ³stico

Cuando ejecutas el modelo, verÃ¡s en la consola:
```
load_tensors: layer 0 assigned to device CPU
load_tensors: layer 1 assigned to device CPU
...
```

Todas las capas se asignan a **CPU** aunque hayas configurado `n_gpu_layers=35`.

## âš ï¸ Por QuÃ© No Funciona

`llama-cpp-python` en Windows necesita ser **compilado** con soporte CUDA, pero:

1. No hay binarios precompilados con CUDA para Windows
2. Compilar requiere tener Visual Studio Build Tools configurado correctamente
3. El error actual: "No CUDA toolset found" indica que CMake no encuentra el compilador CUDA

## ğŸ› ï¸ Soluciones Posibles

### OpciÃ³n 1: Usar Ollama (Recomendado â­)

[Ollama](https://ollama.ai/) soporta GPU automÃ¡ticamente en Windows:

```powershell
# Instalar Ollama desde https://ollama.ai/download
# Luego descargar modelos:
ollama pull llama3.1:8b
ollama pull qwen2.5:3b
```

Ventajas:
- âœ… GPU funciona automÃ¡ticamente
- âœ… MÃ¡s rÃ¡pido que llama-cpp-python
- âœ… API compatible con OpenAI
- âœ… No requiere compilaciÃ³n

### OpciÃ³n 2: Compilar llama-cpp-python con CUDA

Requiere:
1. Visual Studio 2022 Build Tools completo
2. CUDA Toolkit 12.x correctamente configurado
3. Configurar variables de entorno

```powershell
# 1. Asegurarse que nvcc funciona
nvcc --version

# 2. Configurar variables
$env:CUDA_PATH = "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.6"
$env:CUDA_HOME = "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.6"
$env:CMAKE_ARGS = "-DGGML_CUDA=on"

# 3. Compilar
pip uninstall llama-cpp-python -y
pip install llama-cpp-python --force-reinstall --no-cache-dir
```

### OpciÃ³n 3: Usar llama.cpp directamente

Descargar binarios precompilados con CUDA:
https://github.com/ggerganov/llama.cpp/releases

```powershell
# Ejecutar directamente
.\llama-server.exe --model modelo.gguf --n-gpu-layers 35
```

### OpciÃ³n 4: Usar LM Studio (MÃ¡s Simple)

[LM Studio](https://lmstudio.ai/) tiene GPU habilitado por defecto:
- âœ… Interfaz grÃ¡fica
- âœ… GPU automÃ¡tico
- âœ… Servidor API local
- âœ… Sin compilaciÃ³n

## ğŸ“Š Rendimiento Actual

Con CPU (sin GPU):
- ğŸŒ ~2-5 tokens/segundo
- ğŸ’» 100% uso CPU
- ğŸ”‹ Consumo alto

Con GPU (si funcionara):
- âš¡ ~20-50 tokens/segundo
- ğŸ® Uso GPU
- ğŸ”‹ Consumo menor en CPU

## ğŸ¯ RecomendaciÃ³n

Para usar GPU sin complicaciones, te sugiero:

1. **Corto plazo**: Sigue usando CPU (actual) - funciona pero es lento
2. **Mediano plazo**: Instalar **Ollama** - GPU automÃ¡tico, fÃ¡cil de usar
3. **Largo plazo**: Si necesitas control total, compilar llama-cpp-python con CUDA

## ğŸ“ ConfiguraciÃ³n Actual

El cÃ³digo ya estÃ¡ preparado para GPU:
- âœ… `generador_dos_pasos.py` acepta `n_gpu_layers`
- âœ… `api_server.py` pasa el parÃ¡metro
- âœ… UI tiene slider para ajustar capas GPU
- âš ï¸ Solo falta que llama-cpp-python tenga soporte CUDA

Cuando instales una versiÃ³n con GPU, todo funcionarÃ¡ automÃ¡ticamente.

## ğŸ”— Referencias

- llama-cpp-python: https://github.com/abetlen/llama-cpp-python
- llama.cpp releases: https://github.com/ggerganov/llama.cpp/releases
- Ollama: https://ollama.ai/
- LM Studio: https://lmstudio.ai/
