# ğŸ—ï¸ Arquitectura del Sistema de BÃºsqueda IA

> DocumentaciÃ³n tÃ©cnica del sistema de bÃºsqueda hÃ­brida con GPU

---

## ğŸ“Š VisiÃ³n General

El sistema combina **bÃºsqueda semÃ¡ntica** (basada en significado) con **bÃºsqueda por palabras clave** (BM25) para obtener resultados mÃ¡s relevantes.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USUARIO                                   â”‚
â”‚              (Interfaz React - Puerto 5174)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ HTTP POST /api/buscar
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              API REST (Flask + Waitress)                     â”‚
â”‚                   Puerto 5001                                â”‚
â”‚  â€¢ /api/buscar - BÃºsqueda hÃ­brida                           â”‚
â”‚  â€¢ /api/actualizar_indice - Reindexar                       â”‚
â”‚  â€¢ /api/estado - Estado del sistema                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BUSCADOR HÃBRIDO                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ BÃºsqueda        â”‚   70%   â”‚ BÃºsqueda         â”‚   30%   â”‚
â”‚  â”‚ SemÃ¡ntica       â”‚ â”€â”€â”€â”€â”€â–º  â”‚ Palabras Clave   â”‚ â”€â”€â”€â”€â”€â–º  â”‚
â”‚  â”‚ (FAISS)         â”‚         â”‚ (BM25)           â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚           â”‚                           â”‚                    â”‚
â”‚           â”‚    CombinaciÃ³n de scores  â”‚                    â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                        â–¼                                    â”‚
â”‚              Resultados ordenados                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  EMBEDDINGS (GPU)                            â”‚
â”‚              sentence-transformers                           â”‚
â”‚           BAAI/bge-small-en-v1.5                            â”‚
â”‚         NVIDIA RTX 4050 (CUDA 11.8)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§© Componentes Principales

### 1. Frontend (React + Vite)

**Archivo:** `examinator-web/src/App.jsx`

**Funciones clave:**
- `buscarConIA(query)` - EnvÃ­a consulta a backend
- `actualizarIndice(forzar)` - Actualiza/recrea Ã­ndices
- `extraerInfoRelevante(resultado)` - Extrae metadata de resultados
- `cargarEstadoIndice()` - Obtiene estado del sistema

**Puerto:** 5174

**TecnologÃ­as:**
- React 18
- Vite (dev server con HMR)
- Fetch API para comunicaciÃ³n con backend

### 2. Backend API (Flask + Waitress)

**Archivo:** `api_buscador.py`

**Endpoints:**

```python
POST /api/buscar
{
  "query": "texto a buscar",
  "max_resultados": 10,
  "tipo_archivo": "todos"  # o "nota", "flashcard", etc.
}

Response:
{
  "resultados": [
    {
      "nombre_archivo": "archivo.txt",
      "ruta": "/path/to/archivo.txt",
      "score": 0.85,
      "contenido": "snippet del contenido...",
      "tipo": "nota"
    }
  ],
  "total": 5,
  "tiempo": 0.577
}
```

```python
POST /api/actualizar_indice
{
  "forzar": false  # true = reindexar todo
}
```

```python
GET /api/estado
Response:
{
  "gpu_disponible": true,
  "total_chunks": 27,
  "archivos_indexados": 3
}
```

**Servidor:** Waitress (WSGI)
- MÃ¡s estable que Flask dev server en Windows
- No se cierra inesperadamente con CUDA
- Puerto: 5001

### 3. Motor de BÃºsqueda (buscador_ia.py)

**Clase principal:** `BuscadorHibrido`

**Componentes:**

#### a) Indexador (IndexadorLocal)

```python
class IndexadorLocal:
    def indexar_archivos(rutas_archivos):
        # 1. Lee archivos (TXT, PDF, DOCX, JSON)
        # 2. Divide en chunks (800 chars, 200 overlap)
        # 3. Genera embeddings (GPU)
        # 4. Crea Ã­ndices FAISS y BM25
```

**Chunking:**
- TamaÃ±o: 800 caracteres
- Overlap: 200 caracteres
- Motivo: Balance entre contexto y precisiÃ³n

#### b) BÃºsqueda SemÃ¡ntica (FAISS)

```python
def buscar_semantico(query, k=10):
    # 1. Genera embedding de la query (GPU)
    embedding = model.encode([query], device='cuda')
    
    # 2. BÃºsqueda en FAISS (Inner Product)
    scores, indices = faiss_index.search(embedding, k)
    
    # 3. Retorna chunks mÃ¡s similares
    return resultados
```

**Ãndice:** FAISS IndexFlatIP (Inner Product)
- Ventaja: RÃ¡pido para <100k vectores
- DimensiÃ³n: 384 (modelo bge-small)
- MÃ©trica: Similitud coseno (normalizado)

#### c) BÃºsqueda por Palabras Clave (BM25)

```python
def buscar_keywords(query, k=10):
    # 1. Tokeniza query
    tokens = query.lower().split()
    
    # 2. BM25 scoring
    scores = bm25.get_scores(tokens)
    
    # 3. Retorna top-k
    return resultados
```

**Algoritmo:** BM25 Okapi
- Mejora de TF-IDF
- Considera frecuencia de tÃ©rminos y longitud del documento

#### d) CombinaciÃ³n HÃ­brida

```python
def buscar(query, max_resultados=10):
    # 1. BÃºsqueda semÃ¡ntica
    sem_results = buscar_semantico(query, k=20)
    
    # 2. BÃºsqueda keywords
    kw_results = buscar_keywords(query, k=20)
    
    # 3. Combinar scores (70% semÃ¡ntica + 30% keywords)
    final_scores = 0.7 * sem_scores + 0.3 * kw_scores
    
    # 4. Ordenar y retornar top-k
    return sorted(final_scores)[:max_resultados]
```

**Pesos configurables:**
- `peso_semantico = 0.7` - Prioriza significado
- `peso_keywords = 0.3` - Asegura matches exactos

---

## ğŸš€ Flujo de EjecuciÃ³n

### IndexaciÃ³n (crear_indice_inicial.py)

```
1. Escanear carpetas (cursos/, notas/, flashcards/, examenes/)
   â†“
2. Leer archivos (.txt, .json, .pdf, .docx)
   â†“
3. Dividir en chunks (800 chars con 200 overlap)
   â†“
4. Generar embeddings en GPU
   sentence-transformers (BAAI/bge-small-en-v1.5)
   â†“
5. Crear Ã­ndice FAISS
   IndexFlatIP (Inner Product)
   â†“
6. Crear Ã­ndice BM25
   rank-bm25
   â†“
7. Guardar en disco
   indices_busqueda/
   â”œâ”€â”€ faiss_index.bin
   â”œâ”€â”€ bm25_index.pkl
   â””â”€â”€ chunks.json
```

**Tiempo:** ~30 segundos para 100 archivos

### BÃºsqueda (Runtime)

```
Usuario escribe query en frontend
   â†“
POST /api/buscar {"query": "...", "max_resultados": 10}
   â†“
Backend recibe query
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     â”‚                    â”‚
â”‚ BÃºsqueda SemÃ¡ntica  â”‚  BÃºsqueda BM25     â”‚
â”‚ 1. Encode query     â”‚  1. Tokenizar      â”‚
â”‚ 2. FAISS search     â”‚  2. BM25 scoring   â”‚
â”‚ 3. Scores 0-1       â”‚  3. Scores 0-1     â”‚
â”‚                     â”‚                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                     â”‚
           â”‚   Combinar scores   â”‚
           â”‚   (0.7 * sem + 0.3 * kw)
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
              Ordenar por score
                      â†“
          Retornar top-k resultados
                      â†“
      Frontend muestra resultados
      con metadata extraÃ­da
```

**Tiempo:** 0.5-1 segundo con GPU

---

## ğŸ¯ Modelo de Embeddings

**Nombre:** BAAI/bge-small-en-v1.5

**CaracterÃ­sticas:**
- DimensiÃ³n: 384
- TamaÃ±o: ~130MB
- Velocidad: ~1000 textos/segundo (GPU)
- Idioma: InglÃ©s (funciona bien con espaÃ±ol)

**Descarga automÃ¡tica:**
```python
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('BAAI/bge-small-en-v1.5', device='cuda')
# Se descarga en: ~/.cache/huggingface/
```

**Alternativas:**
- `all-MiniLM-L6-v2` - MÃ¡s pequeÃ±o (~80MB)
- `paraphrase-multilingual-MiniLM-L12-v2` - MultilingÃ¼e
- `bge-large-en-v1.5` - MÃ¡s preciso pero mÃ¡s lento

---

## ğŸ’¾ Almacenamiento

### Estructura de Ãndices

```
indices_busqueda/
â”œâ”€â”€ faiss_index.bin        # Vectores FAISS (binario)
â”œâ”€â”€ bm25_index.pkl         # Modelo BM25 (pickle)
â””â”€â”€ chunks.json            # Metadata de chunks
```

**chunks.json:**
```json
[
  {
    "chunk_id": 0,
    "texto": "Contenido del chunk...",
    "archivo": "/ruta/al/archivo.txt",
    "nombre_archivo": "archivo.txt",
    "tipo": "nota",
    "inicio": 0,
    "fin": 800
  }
]
```

**TamaÃ±o estimado:**
- 100 archivos â†’ ~50 chunks â†’ ~20KB (JSON) + ~200KB (FAISS)
- 1000 archivos â†’ ~500 chunks â†’ ~200KB (JSON) + ~2MB (FAISS)

---

## âš¡ Optimizaciones

### GPU Acceleration

```python
# Verificar GPU
import torch
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Cargar modelo en GPU
model = SentenceTransformer(modelo_name, device=device)

# Generar embeddings en GPU
embeddings = model.encode(textos, device=device, show_progress_bar=True)
```

**Speedup:** 5-10x vs CPU

### CachÃ© de Embeddings

Los embeddings se generan una vez durante indexaciÃ³n y se reutilizan en bÃºsquedas.

### Batch Processing

```python
# En vez de:
for texto in textos:
    embedding = model.encode([texto])

# Hacer:
embeddings = model.encode(textos, batch_size=32)
```

---

## ğŸ”§ ConfiguraciÃ³n

**Archivo:** `buscador_ia.py`

```python
class ConfigBuscador:
    modelo_embeddings = 'BAAI/bge-small-en-v1.5'
    chunk_size = 800
    chunk_overlap = 200
    max_resultados = 10
    carpetas = ['cursos', 'notas', 'flashcards', 'examenes']
    extensiones = ['.txt', '.json', '.pdf', '.docx']
```

**Ajustes recomendados:**

| ParÃ¡metro | Valor por defecto | Para mÃ¡s precisiÃ³n | Para mÃ¡s velocidad |
|-----------|-------------------|--------------------|--------------------|
| chunk_size | 800 | 500 | 1200 |
| chunk_overlap | 200 | 300 | 100 |
| peso_semantico | 0.7 | 0.8 | 0.5 |
| peso_keywords | 0.3 | 0.2 | 0.5 |

---

## ğŸ› Debugging

### Ver logs del servidor

```powershell
# Ventana donde corre INICIAR_BUSCADOR_GPU.bat
# Muestra:
Cargando modelo BAAI/bge-small-en-v1.5...
Modelo cargado en: cuda
GPU disponible: NVIDIA GeForce RTX 4050 Laptop GPU
Indices cargados: 27 chunks
Running on http://0.0.0.0:5001
```

### Probar API directamente

```powershell
Invoke-WebRequest -Method POST `
  -Uri "http://localhost:5001/api/buscar" `
  -ContentType "application/json" `
  -Body '{"query":"test","max_resultados":5}' | 
  Select-Object -ExpandProperty Content
```

### Verificar Ã­ndices

```python
import faiss
import pickle

# FAISS
index = faiss.read_index('indices_busqueda/faiss_index.bin')
print(f"Total vectores: {index.ntotal}")

# BM25
with open('indices_busqueda/bm25_index.pkl', 'rb') as f:
    bm25 = pickle.load(f)
    print(f"Documentos: {len(bm25.corpus_size)}")
```

---

## ğŸ“ˆ MÃ©tricas de Rendimiento

**Hardware de referencia:** RTX 4050, 16GB RAM, SSD

| OperaciÃ³n | Tiempo | Notas |
|-----------|--------|-------|
| Indexar 100 archivos | ~30s | GPU |
| Primera bÃºsqueda | ~5s | Carga modelo |
| BÃºsquedas siguientes | ~0.5s | GPU |
| Modo CPU | ~3-5s | Sin GPU |

---

## ğŸ”’ Seguridad

- âœ… CORS limitado a localhost:5174
- âœ… No guarda queries del usuario
- âœ… Solo lee archivos locales
- âœ… No conexiÃ³n a internet (excepto descarga modelo)

---

## ğŸ“š Referencias

- [FAISS Documentation](https://github.com/facebookresearch/faiss)
- [Sentence Transformers](https://www.sbert.net/)
- [BM25 Algorithm](https://en.wikipedia.org/wiki/Okapi_BM25)
- [PyTorch CUDA](https://pytorch.org/get-started/locally/)
