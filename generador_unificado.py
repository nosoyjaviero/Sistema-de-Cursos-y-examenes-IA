"""
Adaptador para usar Ollama o llama-cpp-python de forma transparente
"""
from pathlib import Path
from typing import List, Dict, Optional
import json
import requests
from datetime import datetime
from generador_examenes import PreguntaExamen


class GeneradorUnificado:
    """Generador que puede usar Ollama o llama-cpp-python"""
    
    def __init__(self, usar_ollama: bool = True, modelo_ollama: str = "llama3.2:3b", 
                 modelo_path_gguf: str = None, n_gpu_layers: int = 35):
        self.usar_ollama = usar_ollama
        self.modelo_ollama = modelo_ollama
        # Convertir path relativo a absoluto
        if modelo_path_gguf:
            modelo_path = Path(modelo_path_gguf)
            if not modelo_path.is_absolute():
                modelo_path = Path.cwd() / modelo_path
            self.modelo_path_gguf = str(modelo_path)
        else:
            self.modelo_path_gguf = None
        self.n_gpu_layers = n_gpu_layers
        self.llm = None
        
        # Sistema de logging detallado
        self.log_dir = Path("logs_practicas_detallado")
        self.log_dir.mkdir(exist_ok=True)
        self.current_log_file = None
        self.log_data = {}
        
        if usar_ollama:
            self._verificar_ollama()
        else:
            self._cargar_modelo_gguf()
    
    def _verificar_ollama(self):
        """Verifica que Ollama est√© disponible"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=2)
            if response.status_code == 200:
                modelos = response.json().get('models', [])
                print(f"‚úÖ Ollama activo - {len(modelos)} modelos")
                if not any(m['name'].startswith(self.modelo_ollama.split(':')[0]) for m in modelos):
                    print(f"‚ö†Ô∏è Modelo {self.modelo_ollama} no encontrado")
                    print(f"   Ejecuta: ollama pull {self.modelo_ollama}")
            else:
                print("‚ö†Ô∏è Ollama no responde")
                self.usar_ollama = False
        except Exception as e:
            print(f"‚ùå Ollama no disponible: {e}")
            print("üí° Usando llama-cpp-python como fallback")
            self.usar_ollama = False
            if self.modelo_path_gguf:
                self._cargar_modelo_gguf()
    
    def _cargar_modelo_gguf(self):
        """Carga modelo GGUF con llama-cpp-python"""
        if not self.modelo_path_gguf:
            print("‚ö†Ô∏è No hay modelo GGUF configurado")
            return
        
        try:
            from llama_cpp import Llama
            print(f"üîÑ Cargando GGUF: {self.modelo_path_gguf}")
            self.llm = Llama(
                model_path=self.modelo_path_gguf,
                n_ctx=8192,
                n_threads=6,
                n_gpu_layers=self.n_gpu_layers,
                verbose=False
            )
            gpu_info = f"GPU activada ({self.n_gpu_layers} capas)" if self.n_gpu_layers > 0 else "Solo CPU"
            print(f"‚úÖ Modelo GGUF cargado - {gpu_info}")
        except Exception as e:
            print(f"‚ùå Error cargando GGUF: {e}")
            self.llm = None
    
    def _iniciar_log(self):
        """Inicia un nuevo archivo de log para esta consulta en carpeta √∫nica"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Crear carpeta √∫nica para esta consulta
        carpeta_consulta = self.log_dir / f"practica_{timestamp}"
        carpeta_consulta.mkdir(parents=True, exist_ok=True)
        
        # Archivo de log dentro de la carpeta
        self.current_log_file = carpeta_consulta / f"practica_{timestamp}.log"
        
        self.log_data = {
            "timestamp": timestamp,
            "fecha_hora": datetime.now().isoformat(),
            "request": {},
            "prompt_enviado": "",
            "respuesta_modelo": "",
            "json_extraido": "",
            "preguntas_parseadas": [],
            "filtrado": {},
            "resultado_final": [],
            "errores": []
        }
        print(f"\nüìã Log detallado: {self.current_log_file}")
    
    def _agregar_log(self, seccion: str, datos: dict):
        """Agrega informaci√≥n a una secci√≥n del log"""
        if seccion in self.log_data:
            if isinstance(self.log_data[seccion], dict):
                self.log_data[seccion].update(datos)
            elif isinstance(self.log_data[seccion], list):
                self.log_data[seccion].append(datos)
            else:
                self.log_data[seccion] = datos
        else:
            self.log_data[seccion] = datos
    
    def _guardar_log(self):
        """Guarda el log completo en archivo"""
        if not self.current_log_file:
            return
        
        try:
            # Determinar si fue exitoso o fall√≥
            errores = self.log_data.get('errores', [])
            resultado_final = self.log_data.get('resultado_final', [])
            num_preguntas_solicitadas = sum(self.log_data.get('request', {}).get('num_preguntas', {}).values())
            num_preguntas_generadas = len(resultado_final)
            
            exitoso = len(errores) == 0 and num_preguntas_generadas > 0
            estado = "‚úÖ EXITOSO" if exitoso else "‚ùå FALL√ì"
            
            # Crear versi√≥n legible
            with open(self.current_log_file, 'w', encoding='utf-8') as f:
                f.write("="*80 + "\n")
                f.write("LOG DETALLADO DE GENERACI√ìN DE PR√ÅCTICA\n")
                f.write("="*80 + "\n\n")
                
                # RESUMEN EJECUTIVO
                f.write("üéØ RESUMEN EJECUTIVO\n")
                f.write("-"*80 + "\n")
                f.write(f"Estado: {estado}\n")
                f.write(f"Fecha/Hora: {self.log_data['fecha_hora']}\n")
                f.write(f"Preguntas solicitadas: {num_preguntas_solicitadas}\n")
                f.write(f"Preguntas generadas: {num_preguntas_generadas}\n")
                
                if errores:
                    f.write(f"\n‚ö†Ô∏è ERRORES ENCONTRADOS ({len(errores)}):\n")
                    for i, error in enumerate(errores, 1):
                        f.write(f"  {i}. {error}\n")
                else:
                    f.write(f"\n‚úÖ Sin errores\n")
                
                # Detalles del filtrado si existe
                filtrado = self.log_data.get('filtrado', {})
                if filtrado:
                    f.write(f"\nFiltrado:\n")
                    f.write(f"  ‚Ä¢ Total generadas: {filtrado.get('total_generadas', 0)}\n")
                    f.write(f"  ‚Ä¢ Total filtradas: {filtrado.get('total_filtradas', 0)}\n")
                    contador = filtrado.get('contador_por_tipo', {})
                    if contador:
                        f.write(f"  ‚Ä¢ Por tipo: {contador}\n")
                
                f.write("\n" + "="*80 + "\n\n")
                
                # REQUEST RECIBIDO
                f.write("-"*80 + "\n")
                f.write("1. REQUEST RECIBIDO DEL FRONTEND\n")
                f.write("-"*80 + "\n")
                for key, value in self.log_data.get('request', {}).items():
                    f.write(f"{key}: {value}\n")
                f.write("\n")
                
                # PROMPT ENVIADO AL MODELO
                f.write("-"*80 + "\n")
                f.write("2. PROMPT ENVIADO AL MODELO\n")
                f.write("-"*80 + "\n")
                f.write(f"Longitud: {len(self.log_data.get('prompt_enviado', ''))} caracteres\n\n")
                f.write(self.log_data.get('prompt_enviado', 'No disponible'))
                f.write("\n\n")
                
                # RESPUESTA DEL MODELO
                f.write("-"*80 + "\n")
                f.write("3. RESPUESTA COMPLETA DEL MODELO\n")
                f.write("-"*80 + "\n")
                f.write(f"Longitud: {len(self.log_data.get('respuesta_modelo', ''))} caracteres\n\n")
                f.write(self.log_data.get('respuesta_modelo', 'No disponible'))
                f.write("\n\n")
                
                # JSON EXTRA√çDO
                f.write("-"*80 + "\n")
                f.write("4. JSON EXTRA√çDO Y PARSEADO\n")
                f.write("-"*80 + "\n")
                json_str = self.log_data.get('json_extraido', '')
                if json_str:
                    try:
                        json_obj = json.loads(json_str) if isinstance(json_str, str) else json_str
                        f.write(json.dumps(json_obj, indent=2, ensure_ascii=False))
                    except:
                        f.write(str(json_str))
                else:
                    f.write("No se extrajo JSON")
                f.write("\n\n")
                
                # PREGUNTAS PARSEADAS
                f.write("-"*80 + "\n")
                f.write("5. PREGUNTAS PARSEADAS (Objetos Python)\n")
                f.write("-"*80 + "\n")
                for i, pregunta in enumerate(self.log_data.get('preguntas_parseadas', []), 1):
                    f.write(f"\nPregunta {i}:\n")
                    f.write(json.dumps(pregunta, indent=2, ensure_ascii=False))
                    f.write("\n")
                f.write("\n")
                
                # FILTRADO
                f.write("-"*80 + "\n")
                f.write("6. PROCESO DE FILTRADO\n")
                f.write("-"*80 + "\n")
                filtrado = self.log_data.get('filtrado', {})
                for key, value in filtrado.items():
                    f.write(f"{key}: {value}\n")
                f.write("\n")
                
                # RESULTADO FINAL
                f.write("-"*80 + "\n")
                f.write("7. RESULTADO FINAL DEVUELTO AL FRONTEND\n")
                f.write("-"*80 + "\n")
                f.write(f"Total preguntas: {len(self.log_data.get('resultado_final', []))}\n\n")
                for i, pregunta in enumerate(self.log_data.get('resultado_final', []), 1):
                    f.write(f"\nPregunta {i}:\n")
                    f.write(json.dumps(pregunta, indent=2, ensure_ascii=False))
                    f.write("\n")
                f.write("\n")
                
                # ERRORES
                if self.log_data.get('errores'):
                    f.write("-"*80 + "\n")
                    f.write("8. ERRORES ENCONTRADOS\n")
                    f.write("-"*80 + "\n")
                    for error in self.log_data['errores']:
                        f.write(f"‚Ä¢ {error}\n")
                    f.write("\n")
                
                f.write("="*80 + "\n")
                f.write("FIN DEL LOG\n")
                f.write("="*80 + "\n")
            
            # Guardar tambi√©n versi√≥n JSON
            json_file = self.current_log_file.with_suffix('.json')
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(self.log_data, f, indent=2, ensure_ascii=False)
            
            print(f"‚úÖ Log guardado: {self.current_log_file}")
            print(f"‚úÖ JSON guardado: {json_file}")
            
        except Exception as e:
            print(f"‚ùå Error guardando log: {e}")
    
    def _generar_ollama(self, prompt: str, max_tokens: int, temperature: float) -> str:
        """Genera con Ollama"""
        try:
            # Detectar modelos lentos y ajustar solo el timeout
            es_deepseek = 'deepseek' in self.modelo_ollama.lower()
            
            # Determinar si se usa GPU basado en n_gpu_layers
            usar_gpu = self.n_gpu_layers > 0
            modo_gpu = "GPU activada" if usar_gpu else "Solo CPU"
            
            print(f"\n{'='*60}")
            print(f"üéÆ Modelo Ollama: {self.modelo_ollama}")
            print(f"‚öôÔ∏è  Configuraci√≥n:")
            print(f"   ‚Ä¢ Modo: {modo_gpu}")
            print(f"   ‚Ä¢ Temperature: {temperature}")
            print(f"   ‚Ä¢ Max tokens: {max_tokens}")
            print(f"   ‚Ä¢ Prompt length: {len(prompt)} caracteres")
            
            if es_deepseek:
                print(f"   ‚Ä¢ Modelo de razonamiento: DeepSeek-R1")
                print(f"   ‚Ä¢ Genera razonamiento interno antes de responder")
            print(f"{'='*60}\n")
            
            # Debug: mostrar inicio del prompt
            print(f"üìù INICIO DEL PROMPT (primeros 500 chars):")
            print(f"{prompt[:500]}")
            print(f"...\n")
            
            # Timeout ajustado seg√∫n modelo (sin cambiar max_tokens)
            if es_deepseek:
                timeout_segundos = 1800  # 30 minutos para DeepSeek-R1
                print(f"‚è±Ô∏è  Timeout configurado: {timeout_segundos} segundos (30 minutos)")
                print(f"üí° DeepSeek-R1 hace razonamiento complejo y puede tardar mucho")
                print(f"üí° Vale la pena esperar por la calidad de sus respuestas...")
            else:
                timeout_segundos = 600  # 10 minutos para otros modelos
                print(f"‚è±Ô∏è  Timeout configurado: {timeout_segundos} segundos (10 minutos)")
                print(f"üí° Modelos grandes pueden tardar varios minutos...")
            print(f"üí° Modelos grandes pueden tardar varios minutos...")
            print(f"üöÄ Enviando request a Ollama...\n")
            
            # Ajustar prompt para DeepSeek-R1 (permitir razonamiento, pero pedir JSON al final)
            prompt_final = prompt
            if es_deepseek:
                # DeepSeek-R1 es un modelo de razonamiento - dejarlo razonar pero pedir JSON al final
                prompt_final = f"""{prompt}

IMPORTANTE: Despu√©s de tu an√°lisis, DEBES generar el JSON v√°lido con la estructura solicitada.
El JSON debe comenzar con {{ y terminar con }}.
Puedes razonar primero, pero al final SIEMPRE incluye el JSON completo."""
                print(f"üìù Prompt adaptado para DeepSeek-R1 (permite razonamiento + JSON al final)\n")
            
            # Configurar opciones seg√∫n modo GPU/CPU
            opciones = {
                "temperature": temperature,
                "num_predict": max_tokens,
                "stop": ["<|eot_id|>", "<|end_of_text|>", "\n\n\n"]
            }
            
            # Si n_gpu_layers es 0, forzar uso de CPU
            if not usar_gpu:
                opciones["num_gpu"] = 0
                print(f"üî∑ Modo CPU forzado (num_gpu=0)\n")
            
            response = requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": self.modelo_ollama,
                    "prompt": prompt_final,
                    "stream": False,
                    "options": opciones
                },
                timeout=timeout_segundos
            )
            
            print(f"üì¨ Response status: {response.status_code}")
            
            if response.status_code == 200:
                print(f"‚úÖ Generaci√≥n completada ({modo_gpu})\n")
                respuesta_json = response.json()
                respuesta_completa = respuesta_json.get('response', '')
                
                if not respuesta_completa:
                    print(f"‚ö†Ô∏è ADVERTENCIA: Respuesta vac√≠a")
                    print(f"   JSON completo: {respuesta_json}")
                    return None
                
                # Debug: Guardar respuesta completa
                print(f"üìù Longitud de respuesta: {len(respuesta_completa)} caracteres")
                print(f"üìÑ Primeros 500 caracteres:\n{respuesta_completa[:500]}\n")
                if len(respuesta_completa) > 500:
                    print(f"üìÑ √öltimos 500 caracteres:\n{respuesta_completa[-500:]}\n")
                
                return respuesta_completa
            else:
                error_detail = response.text if response.text else "Sin detalles"
                print(f"‚ùå Error Ollama {response.status_code}")
                print(f"   Detalles: {error_detail[:500]}")
                return None
        except requests.exceptions.Timeout:
            print(f"‚è±Ô∏è TIMEOUT: La generaci√≥n excedi√≥ {timeout_segundos} segundos")
            print(f"   Considera usar menos preguntas o un modelo m√°s peque√±o")
            return None
        except Exception as e:
            print(f"‚ùå Error Ollama: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _generar_ollama_chat(self, messages: list, max_tokens: int, temperature: float) -> str:
        """Genera con Ollama usando API de chat (mantiene historial/contexto)"""
        try:
            # Determinar si se usa GPU basado en n_gpu_layers
            usar_gpu = self.n_gpu_layers > 0
            modo_gpu = "GPU activada" if usar_gpu else "Solo CPU"
            
            print(f"\n{'='*60}")
            print(f"üí¨ CHAT CON CONTEXTO - Modelo Ollama: {self.modelo_ollama}")
            print(f"‚öôÔ∏è  Configuraci√≥n:")
            print(f"   ‚Ä¢ Modo: {modo_gpu}")
            print(f"   ‚Ä¢ Temperature: {temperature}")
            print(f"   ‚Ä¢ Max tokens: {max_tokens}")
            print(f"   ‚Ä¢ Mensajes en historial: {len(messages)}")
            print(f"{'='*60}\n")
            
            # Debug: mostrar estructura de mensajes
            print(f"üìú Estructura del chat:")
            for i, msg in enumerate(messages):
                role = msg.get('role', 'unknown')
                content_preview = msg.get('content', '')[:100]
                print(f"   {i+1}. {role}: {content_preview}...")
            print()
            
            # Timeout m√°s largo para modelos grandes
            timeout_segundos = 600  # 10 minutos
            print(f"‚è±Ô∏è  Timeout configurado: {timeout_segundos} segundos")
            print(f"üöÄ Enviando request a Ollama API de chat...\n")
            
            # Configurar opciones seg√∫n modo GPU/CPU
            opciones = {
                "temperature": temperature,
                "num_predict": max_tokens,
                "stop": ["<|eot_id|>", "<|end_of_text|>"]
            }
            
            # Si n_gpu_layers es 0, forzar uso de CPU
            if not usar_gpu:
                opciones["num_gpu"] = 0
                print(f"üî∑ Modo CPU forzado (num_gpu=0)\n")
            
            response = requests.post(
                "http://localhost:11434/api/chat",
                json={
                    "model": self.modelo_ollama,
                    "messages": messages,
                    "stream": False,
                    "options": opciones
                },
                timeout=timeout_segundos
            )
            
            print(f"üì¨ Response status: {response.status_code}")
            
            if response.status_code == 200:
                print(f"‚úÖ Generaci√≥n completada ({modo_gpu} y contexto)\n")
                respuesta_json = response.json()
                
                # La API de chat devuelve el mensaje en un formato diferente
                mensaje_respuesta = respuesta_json.get('message', {})
                respuesta_completa = mensaje_respuesta.get('content', '')
                
                if not respuesta_completa:
                    print(f"‚ö†Ô∏è ADVERTENCIA: Respuesta vac√≠a")
                    print(f"   JSON completo: {respuesta_json}")
                    return None
                
                # Debug: Guardar respuesta completa
                print(f"üìù Longitud de respuesta: {len(respuesta_completa)} caracteres")
                print(f"üìÑ Primeros 300 caracteres:\n{respuesta_completa[:300]}\n")
                
                return respuesta_completa
            else:
                error_detail = response.text if response.text else "Sin detalles"
                print(f"‚ùå Error Ollama {response.status_code}")
                print(f"   Detalles: {error_detail[:500]}")
                return None
        except requests.exceptions.Timeout:
            print(f"‚è±Ô∏è TIMEOUT: La generaci√≥n excedi√≥ {timeout_segundos} segundos")
            print(f"   Considera reducir el historial o usar un modelo m√°s peque√±o")
            return None
        except Exception as e:
            print(f"‚ùå Error Ollama Chat: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _generar_gguf(self, prompt: str, max_tokens: int, temperature: float, 
                     top_p: float, repeat_penalty: float) -> str:
        """Genera con llama-cpp-python"""
        if not self.llm:
            return None
        
        try:
            resp = self.llm(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                repeat_penalty=repeat_penalty,
                stop=["<|eot_id|>", "<|end_of_text|>", "```"]
            )
            return resp['choices'][0]['text']
        except Exception as e:
            print(f"‚ùå Error GGUF: {e}")
            return None
    
    def generar_examen(self, contenido_documento: str, 
                      num_preguntas: Dict[str, int] = None,
                      callback_progreso = None,
                      ajustes_modelo: dict = None,
                      archivos: list = None,
                      session_id: str = None,
                      sin_prompt_sistema: bool = False,
                      tipo_caso: str = None) -> List[PreguntaExamen]:
        """Genera examen usando Ollama o GGUF
        sin_prompt_sistema: Si es True, usa el contenido directamente sin agregar instrucciones
        tipo_caso: Para casos de estudio, especifica el tipo (descriptivo, analitico, resolucion, etc.)
        """
        
        # INICIAR LOG DETALLADO
        self._iniciar_log()
        
        if num_preguntas is None:
            num_preguntas = {'multiple': 6, 'verdadero_falso': 4, 'corta': 2}
        
        if ajustes_modelo is None:
            ajustes_modelo = {
                'temperature': 0.25,
                'max_tokens': 3000,
                'top_p': 0.9,
                'repeat_penalty': 1.15
            }
        
        # Registrar request
        self._agregar_log('request', {
            'num_preguntas': num_preguntas,
            'ajustes_modelo': ajustes_modelo,
            'sin_prompt_sistema': sin_prompt_sistema,
            'tipo_caso': tipo_caso,
            'usar_ollama': self.usar_ollama,
            'modelo': self.modelo_ollama if self.usar_ollama else self.modelo_path_gguf,
            'contenido_length': len(contenido_documento)
        })
        
        if callback_progreso:
            callback_progreso(15, "Preparando prompt...")
        
        # Verificar que contenido_documento es un string
        if not isinstance(contenido_documento, str):
            error_msg = f"contenido_documento no es string, es {type(contenido_documento)}"
            print(f"‚ùå ERROR: {error_msg}")
            self._agregar_log('errores', error_msg)
            self._guardar_log()
            raise TypeError(f"contenido_documento debe ser string, recibido: {type(contenido_documento)}")
        
        # Crear prompt
        contenido_corto = contenido_documento[:8000]
        total = sum(num_preguntas.values())
        
        # IMPORTANTE: Si hay casos de estudio, SIEMPRE usar _crear_prompt
        # porque necesita las instrucciones detalladas del tipo de caso
        tiene_casos = num_preguntas.get('case_study', 0) > 0 or num_preguntas.get('caso_estudio', 0) > 0
        
        if tiene_casos:
            # CASOS DE ESTUDIO: Usar SIEMPRE el formato estructurado detallado
            # Extraer solo el CONTENIDO real (despu√©s de "CONTENIDO:")
            if "CONTENIDO:" in contenido_documento:
                # El prompt del usuario tiene formato "prompt_personalizado\n\nCONTENIDO:\ncontenido_real"
                partes = contenido_documento.split("CONTENIDO:", 1)
                if len(partes) > 1:
                    contenido_corto = partes[1].strip()[:8000]
            
            prompt = self._crear_prompt(contenido_corto, num_preguntas, total, tipo_caso)
            print(f"üéØ CASOS DE ESTUDIO DETECTADOS: Usando prompt estructurado con tipo '{tipo_caso}'")
            print(f"   Contenido extra√≠do: {len(contenido_corto)} caracteres")
        elif sin_prompt_sistema:
            # Modo prompt personalizado: usar contenido directamente (solo si NO hay casos de estudio)
            prompt = contenido_corto
            print(f"üéØ MODO PROMPT PERSONALIZADO: Usando prompt del usuario directamente")
        else:
            # Modo normal: agregar formato del sistema
            prompt = self._crear_prompt(contenido_corto, num_preguntas, total, tipo_caso)
        
        # Registrar prompt
        self._agregar_log('prompt_enviado', prompt)
        
        if callback_progreso:
            motor = "Ollama + GPU" if self.usar_ollama else "llama-cpp-python"
            callback_progreso(25, f"Generando con {motor}...")
        
        # Generar
        print(f"\n{'='*60}")
        print(f"ü§ñ Generando {total} preguntas con IA...")
        print(f"{'='*60}")
        
        if self.usar_ollama:
            respuesta = self._generar_ollama(
                prompt, 
                ajustes_modelo['max_tokens'], 
                ajustes_modelo['temperature']
            )
        else:
            respuesta = self._generar_gguf(
                prompt,
                ajustes_modelo['max_tokens'],
                ajustes_modelo['temperature'],
                ajustes_modelo['top_p'],
                ajustes_modelo['repeat_penalty']
            )
        
        if not respuesta:
            error_msg = "No se obtuvo respuesta del modelo"
            print(f"‚ùå {error_msg}")
            self._agregar_log('errores', error_msg)
            self._guardar_log()
            return []
        
        # Registrar respuesta del modelo
        self._agregar_log('respuesta_modelo', respuesta)
        
        if callback_progreso:
            callback_progreso(70, "Procesando respuesta...")
        
        # Parsear JSON
        preguntas = self._extraer_preguntas(respuesta, num_preguntas)
        
        if callback_progreso:
            callback_progreso(100, f"¬°{len(preguntas)} preguntas generadas!")
        
        return preguntas
    
    def _crear_prompt(self, contenido: str, num_preguntas: Dict[str, int], total: int, tipo_caso: str = None) -> str:
        """Crea el prompt optimizado
        tipo_caso: Para casos de estudio, especifica el tipo (descriptivo, analitico, resolucion, etc.)
        """
        # Construir lista detallada de tipos de preguntas (USANDO NOMBRES NORMALIZADOS)
        tipos_detalle = []
        if num_preguntas.get('mcq', 0) > 0:
            tipos_detalle.append(f"{num_preguntas['mcq']} de opci√≥n m√∫ltiple (4 opciones A/B/C/D, puntos: 3)")
        if num_preguntas.get('true_false', 0) > 0:
            tipos_detalle.append(f"{num_preguntas['true_false']} verdadero/falso (puntos: 2)")
        if num_preguntas.get('short_answer', 0) > 0:
            tipos_detalle.append(f"{num_preguntas['short_answer']} de respuesta corta (puntos: 3)")
        if num_preguntas.get('open_question', 0) > 0:
            tipos_detalle.append(f"{num_preguntas['open_question']} de desarrollo/ensayo (puntos: 5)")
        
        # Soporte para case_study
        num_casos = num_preguntas.get('case_study', 0)
        if num_casos > 0:
            tipo_desc = f" ({tipo_caso})" if tipo_caso else ""
            tipos_detalle.append(f"{num_casos} caso(s) de estudio{tipo_desc} (puntos: 10)")
        
        tipos_str = "\n".join([f"{i+1}. {t}" for i, t in enumerate(tipos_detalle)])
        
        # Determinar si hay casos de estudio y obtener el prompt espec√≠fico
        caso_estudio_prompt = ""
        if num_casos > 0 and tipo_caso:
            caso_estudio_prompt = self._obtener_prompt_caso_estudio(tipo_caso)
        
        # Formato JSON base (USANDO NOMBRES NORMALIZADOS)
        json_ejemplos = []
        
        # Agregar ejemplos seg√∫n tipos solicitados
        if num_preguntas.get('mcq', 0) > 0:
            json_ejemplos.append("""    {
      "tipo": "mcq",
      "pregunta": "¬øPregunta clara y espec√≠fica sobre el contenido?",
      "opciones": ["A) Primera opci√≥n", "B) Segunda opci√≥n", "C) Tercera opci√≥n", "D) Cuarta opci√≥n"],
      "respuesta_correcta": "A",
      "puntos": 3
    }""")
        
        if num_preguntas.get('true_false', 0) > 0:
            json_ejemplos.append("""    {
      "tipo": "true_false",
      "pregunta": "Afirmaci√≥n clara basada en el contenido",
      "respuesta_correcta": "verdadero",
      "puntos": 2
    }""")
        
        if num_preguntas.get('short_answer', 0) > 0:
            json_ejemplos.append("""    {
      "tipo": "short_answer",
      "pregunta": "Pregunta que requiere una respuesta breve y concreta",
      "respuesta_correcta": "Respuesta esperada (2-3 oraciones)",
      "puntos": 3
    }""")
        
        if num_preguntas.get('open_question', 0) > 0:
            json_ejemplos.append("""    {
      "tipo": "open_question",
      "pregunta": "Pregunta que requiere an√°lisis profundo y desarrollo extenso",
      "respuesta_correcta": "Respuesta esperada detallada con conceptos clave, explicaciones y ejemplos",
      "puntos": 5
    }""")
        
        # Soporte para case_study
        if num_casos > 0:
            json_ejemplos.append(caso_estudio_prompt)
        
        json_ejemplos_str = ",\n".join(json_ejemplos)
        
        return f"""Eres un experto en crear ex√°menes educativos. Tu tarea es generar EXACTAMENTE {total} preguntas REALES basadas en el contenido proporcionado.

CONTENIDO A EVALUAR:
{contenido}

IMPORTANTE - DEBES GENERAR {total} PREGUNTAS COMPLETAS:
{tipos_str}

‚ö†Ô∏è REGLAS CR√çTICAS:
1. Genera EXACTAMENTE {total} preguntas COMPLETAS con contenido REAL
2. NO uses placeholders como "...", "[...]", "puntos: ..."
3. CADA pregunta debe estar COMPLETAMENTE llena con:
   - "tipo": uno de estos valores exactos: "mcq", "true_false", "short_answer", "open_question"
   - "pregunta": texto completo de la pregunta (m√≠nimo 10 palabras)
   - Para "mcq": "opciones" debe ser un array de 4 strings completos (ej: ["A) opci√≥n real 1", "B) opci√≥n real 2", "C) opci√≥n real 3", "D) opci√≥n real 4"])
   - "respuesta_correcta": la respuesta correcta REAL (para MCQ: letra A/B/C/D, para otros: texto completo)
   - "puntos": n√∫mero entero (3 para mcq, 2 para true_false, 3 para short_answer, 5 para open_question)
4. Todas las preguntas deben basarse en informaci√≥n del contenido proporcionado
5. NO inventes informaci√≥n que no est√© en el texto
6. Responde SOLO con JSON v√°lido, sin c√≥digo markdown, sin explicaciones adicionales

FORMATO JSON V√ÅLIDO (con datos REALES, NO placeholders):
{{
  "preguntas": [
    {{
      "tipo": "mcq",
      "pregunta": "¬øSeg√∫n el contenido, cu√°l es la diferencia principal entre arte y dise√±o?",
      "opciones": ["A) El arte es un sustantivo y el dise√±o es un verbo", "B) No hay diferencia", "C) El arte es comercial", "D) El dise√±o no comunica"],
      "respuesta_correcta": "A",
      "puntos": 3
    }},
    {{
      "tipo": "short_answer",
      "pregunta": "Explica brevemente qu√© significa HCI seg√∫n la clase",
      "respuesta_correcta": "HCI significa Human-Computer Interaction (Interacci√≥n Humano-Computadora), que estudia c√≥mo las personas interact√∫an con la tecnolog√≠a",
      "puntos": 3
    }}
  ]
}}

AHORA GENERA LAS {total} PREGUNTAS COMPLETAS CON DATOS REALES:"""
    
    def _obtener_prompt_caso_estudio(self, tipo_caso: str) -> str:
        """Retorna el formato JSON espec√≠fico para cada tipo de caso de estudio
        
        IMPORTANTE: Los casos de estudio requieren MUCHO M√ÅS DETALLE que otros tipos de preguntas.
        - El contexto debe tener al menos 3-5 oraciones (100-200 palabras)
        - La descripci√≥n debe tener al menos 4-6 oraciones (150-300 palabras)
        - Todos los arrays deben tener 4-6 elementos detallados
        """
    
    def _filtrar_preguntas(self, preguntas: List[PreguntaExamen], num_preguntas: Dict[str, int]) -> List[PreguntaExamen]:
        """Filtra preguntas por tipo y cantidad solicitada
        
        ESTRATEGIA DE FILTRADO:
        1. Prioriza cumplir con las cantidades exactas por tipo
        2. Si faltan algunos tipos, incluye extras de otros tipos hasta alcanzar el total solicitado
        """
        preguntas_filtradas = []
        contador_por_tipo = {}
        preguntas_sobrantes = []
        
        # Mapeo de tipos nuevos a tipos del sistema
        mapeo_tipos = {
            'flashcard': 'flashcard',
            'mcq': 'mcq', 
            'true_false': 'true_false',
            'verdadero_falso': 'true_false',
            'cloze': 'cloze',
            'short_answer': 'short_answer',
            'respuesta_corta': 'short_answer',
            'open_question': 'open_question',
            'desarrollo': 'open_question',
            'case_study': 'case_study',
            'caso_estudio': 'case_study',
            'reading_comprehension': 'reading_comprehension',
            'reading_true_false': 'reading_true_false',
            'reading_cloze': 'reading_cloze',
            'reading_skill': 'reading_skill',
            'reading_matching': 'reading_matching',
            'reading_sequence': 'reading_sequence',
            'writing_short': 'writing_short',
            'writing_paraphrase': 'writing_paraphrase',
            'writing_correction': 'writing_correction',
            'writing_transformation': 'writing_transformation',
            'writing_essay': 'writing_essay',
            'writing_sentence_builder': 'writing_sentence_builder',
            'writing_picture_description': 'writing_picture_description',
            'writing_email': 'writing_email',
            'multiple': 'mcq',
            'corta': 'short_answer'
        }
        
        # FASE 1: Seleccionar preguntas seg√∫n cantidades solicitadas por tipo
        for pregunta in preguntas:
            # DEBUG: Imprimir tipo exacto de la pregunta
            print(f"  üîç Pregunta tipo='{pregunta.tipo}' (repr: {repr(pregunta.tipo)})")
            tipo_normalizado = mapeo_tipos.get(pregunta.tipo, pregunta.tipo)
            print(f"     ‚Üí Normalizado a: '{tipo_normalizado}'")
            cantidad_solicitada = num_preguntas.get(tipo_normalizado, 0)
            print(f"     ‚Üí Cantidad solicitada de '{tipo_normalizado}': {cantidad_solicitada}")
            
            if cantidad_solicitada > 0:
                if tipo_normalizado not in contador_por_tipo:
                    contador_por_tipo[tipo_normalizado] = 0
                
                if contador_por_tipo[tipo_normalizado] < cantidad_solicitada:
                    preguntas_filtradas.append(pregunta)
                    contador_por_tipo[tipo_normalizado] += 1
                else:
                    # Guardar extras para usar si faltan otros tipos
                    preguntas_sobrantes.append(pregunta)
        
        # Calcular total esperado
        total_esperado = sum(num_preguntas.values())
        total_actual = len(preguntas_filtradas)
        
        # FASE 2: Si faltan preguntas, agregar extras hasta completar el total
        if total_actual < total_esperado and preguntas_sobrantes:
            faltantes = total_esperado - total_actual
            print(f"‚ö†Ô∏è Faltan {faltantes} preguntas. Agregando extras de otros tipos...")
            
            for i, pregunta_extra in enumerate(preguntas_sobrantes):
                if len(preguntas_filtradas) >= total_esperado:
                    break
                preguntas_filtradas.append(pregunta_extra)
                tipo_norm = mapeo_tipos.get(pregunta_extra.tipo, pregunta_extra.tipo)
                contador_por_tipo[tipo_norm] = contador_por_tipo.get(tipo_norm, 0) + 1
                print(f"   ‚ûï Agregada pregunta extra #{i+1} (tipo: {tipo_norm})")
        
        print(f"üîç Filtrado: {len(preguntas)} generadas ‚Üí {len(preguntas_filtradas)} retornadas (esperadas: {total_esperado})")
        print(f"   Solicitadas: {num_preguntas}")
        print(f"   Filtradas por tipo: {contador_por_tipo}")
        
        # Registrar filtrado
        self._agregar_log('filtrado', {
            'total_generadas': len(preguntas),
            'total_filtradas': len(preguntas_filtradas),
            'total_esperado': total_esperado,
            'solicitadas': num_preguntas,
            'contador_por_tipo': contador_por_tipo
        })
        
        # Verificar si faltaron preguntas
        tipos_faltantes = []
        for tipo, cantidad in num_preguntas.items():
            generadas = contador_por_tipo.get(tipo, 0)
            if generadas < cantidad:
                tipos_faltantes.append(f"{tipo}: {generadas}/{cantidad}")
        
        if tipos_faltantes:
            warning_msg = f"El modelo no gener√≥ suficientes preguntas: {', '.join(tipos_faltantes)}"
            self._agregar_log('errores', warning_msg)
            print(f"‚ö†Ô∏è ADVERTENCIA: El modelo no gener√≥ suficientes preguntas de algunos tipos:")
            for faltante in tipos_faltantes:
                print(f"   - {faltante}")
            if len(preguntas_filtradas) < total_esperado:
                print(f"   üí° Se retornaron {len(preguntas_filtradas)}/{total_esperado} preguntas")
                print(f"   üí° Intenta regenerar la pr√°ctica o reduce la cantidad de tipos solicitados")
            else:
                print(f"   ‚úÖ Se compens√≥ con extras: {len(preguntas_filtradas)}/{total_esperado} preguntas retornadas")
        
        return preguntas_filtradas
    
    def _obtener_prompt_caso_estudio(self, tipo_caso: str) -> str:
        """Retorna el formato JSON espec√≠fico para cada tipo de caso de estudio
        
        IMPORTANTE: Los casos de estudio requieren MUCHO M√ÅS DETALLE que otros tipos de preguntas.
        - El contexto debe tener al menos 3-5 oraciones (100-200 palabras)
        - La descripci√≥n debe tener al menos 4-6 oraciones (150-300 palabras)
        - Todos los arrays deben tener 4-6 elementos detallados
        """
        
        formatos_casos = {
            "descriptivo": """    {
      "tipo": "case_study",
      "subtipo": "descriptivo",
      "titulo": "T√≠tulo descriptivo del caso (m√°x 10 palabras)",
      "contexto": "CONTEXTO DETALLADO (m√≠nimo 100 palabras): Situaci√≥n completa del caso con fecha, lugar, personas involucradas, empresa/organizaci√≥n, industria, antecedentes hist√≥ricos y estado inicial. Describe el escenario completo para que el estudiante entienda el panorama general.",
      "descripcion": "DESCRIPCI√ìN EXHAUSTIVA (m√≠nimo 150 palabras): Descripci√≥n muy detallada de todos los eventos cronol√≥gicamente, personas clave con sus roles, decisiones tomadas con justificaciones, acciones realizadas, resultados obtenidos, reacciones de stakeholders, m√©tricas relevantes, cambios observados. Incluye datos espec√≠ficos, n√∫meros, porcentajes cuando sea posible.",
      "pregunta": "Pregunta espec√≠fica que requiere identificar y analizar los elementos clave que caracterizaron esta situaci√≥n",
      "puntos_clave": ["Punto clave 1 con detalles espec√≠ficos", "Punto clave 2 con contexto", "Punto clave 3 con implicaciones", "Punto clave 4 con evidencia", "Punto clave 5 para an√°lisis profundo"],
      "respuesta_esperada": "An√°lisis descriptivo esperado (m√≠nimo 100 palabras) de los elementos principales del caso con observaciones, s√≠ntesis y entendimiento del contexto",
      "puntos": 10
    }""",
            
            "analitico": """    {
      "tipo": "case_study",
      "subtipo": "analitico",
      "titulo": "T√≠tulo del caso anal√≠tico (m√°x 10 palabras)",
      "contexto": "CONTEXTO DETALLADO (m√≠nimo 100 palabras): Situaci√≥n empresarial, t√©cnica o de negocio completa con antecedentes, estado actual, m√©tricas relevantes, actores involucrados, mercado, competencia, recursos disponibles. Proporciona todos los elementos necesarios para un an√°lisis profundo.",
      "descripcion": "DESCRIPCI√ìN EXHAUSTIVA (m√≠nimo 150 palabras): Descripci√≥n completa de la problem√°tica incluyendo datos cuantitativos y cualitativos, tendencias observadas, se√±ales de alerta, informaci√≥n hist√≥rica, comparativas con per√≠odos anteriores, feedback de usuarios/clientes, m√©tricas de rendimiento, puntos de fricci√≥n identificados. S√© muy espec√≠fico con n√∫meros y fechas.",
      "pregunta": "Analiza en profundidad las causas ra√≠z, las relaciones causales entre factores y las consecuencias directas e indirectas de esta situaci√≥n",
      "areas_analisis": ["Causas principales con evidencia espec√≠fica", "Relaciones causales entre factores A y B", "Consecuencias inmediatas observadas", "Impacto a mediano plazo en X √°rea", "Impacto a largo plazo en sostenibilidad", "Factores externos que influyeron"],
      "respuesta_esperada": "An√°lisis profundo (m√≠nimo 150 palabras) de causas-efectos con relaciones causales claramente identificadas, evidencia para cada afirmaci√≥n y conclusiones fundamentadas",
      "puntos": 10
    }""",
            
            "resolucion": """    {
      "tipo": "case_study",
      "subtipo": "resolucion",
      "titulo": "T√≠tulo del problema (m√°x 10 palabras)",
      "contexto": "CONTEXTO DETALLADO (m√≠nimo 100 palabras): Situaci√≥n problem√°tica completa con antecedentes de c√≥mo surgi√≥ el problema, qui√©nes est√°n afectados, desde cu√°ndo existe, intentos previos de soluci√≥n, estado cr√≠tico actual, urgencia del problema, recursos disponibles para resolverlo.",
      "descripcion": "DESCRIPCI√ìN EXHAUSTIVA (m√≠nimo 150 palabras): Detalles completos del problema incluyendo s√≠ntomas espec√≠ficos, impacto cuantificado en m√©tricas de negocio, afectaci√≥n a diferentes stakeholders, costos del problema, riesgos si no se resuelve, interdependencias con otros sistemas/procesos, evidencia documental del problema, quejas espec√≠ficas recibidas.",
      "pregunta": "Prop√≥n una soluci√≥n viable, detallada y pr√°ctica para resolver este problema considerando todas las restricciones",
      "restricciones": ["Restricci√≥n presupuestaria: m√°ximo $X disponible", "Restricci√≥n de tiempo: debe resolverse en Y semanas", "Restricci√≥n t√©cnica: compatibilidad con sistema Z", "Restricci√≥n de recursos humanos: solo N personas disponibles", "Restricci√≥n regulatoria: cumplir norma ABC"],
      "criterios_evaluacion": ["Viabilidad t√©cnica y factibilidad", "Relaci√≥n costo-beneficio y ROI esperado", "Facilidad y rapidez de implementaci√≥n", "Impacto positivo medible en m√©tricas clave", "Sostenibilidad a largo plazo", "Aceptaci√≥n de stakeholders"],
      "respuesta_esperada": "Soluci√≥n detallada (m√≠nimo 150 palabras) con pasos concretos numerados, cronograma estimado, recursos necesarios, responsables, KPIs de √©xito y justificaci√≥n de por qu√© esta soluci√≥n es la √≥ptima",
      "puntos": 10
    }""",
            
            "decision": """    {
      "tipo": "case_study",
      "subtipo": "decision",
      "titulo": "T√≠tulo de la decisi√≥n cr√≠tica (m√°x 10 palabras)",
      "contexto": "CONTEXTO DETALLADO (m√≠nimo 100 palabras): Escenario completo donde se requiere tomar una decisi√≥n cr√≠tica con impacto significativo. Incluye antecedentes de la situaci√≥n, presiones externas, plazos l√≠mite, stakeholders con sus intereses conflictivos, recursos disponibles, informaci√≥n conocida e incertidumbres.",
      "descripcion": "DESCRIPCI√ìN EXHAUSTIVA (m√≠nimo 150 palabras): Detalles completos del escenario de decisi√≥n incluyendo qui√©nes son todos los stakeholders afectados y sus intereses espec√≠ficos, qu√© opciones est√°n disponibles con sus caracter√≠sticas principales, cu√°les son los trade-offs entre opciones, qu√© informaci√≥n falta, cu√°les son los riesgos de cada camino, precedentes hist√≥ricos de decisiones similares, presiones pol√≠ticas/sociales.",
      "pregunta": "¬øQu√© decisi√≥n tomar√≠as entre las opciones disponibles y por qu√© es la mejor opci√≥n considerando todos los factores?",
      "opciones_disponibles": ["Opci√≥n A: descripci√≥n detallada de esta alternativa con pros, contras y consecuencias esperadas", "Opci√≥n B: descripci√≥n completa de segunda alternativa con impactos cuantificados", "Opci√≥n C: tercera alternativa con an√°lisis de viabilidad", "Opci√≥n D: cuarta opci√≥n con riesgos asociados"],
      "criterios_decision": ["Impacto financiero a corto y largo plazo", "Alineaci√≥n con objetivos estrat√©gicos", "Riesgos y mitigaciones posibles", "Tiempo de implementaci√≥n requerido", "Aceptaci√≥n de stakeholders clave", "Sostenibilidad y escalabilidad"],
      "respuesta_esperada": "Decisi√≥n justificada (m√≠nimo 150 palabras) con an√°lisis detallado de pros y contras de cada opci√≥n, razonamiento l√≥gico, evidencia que soporta la decisi√≥n, plan de mitigaci√≥n de riesgos y criterios de √©xito",
      "puntos": 10
    }""",
            
            "comparativo": """    {
      "tipo": "case_study",
      "subtipo": "comparativo",
      "titulo": "T√≠tulo de la comparaci√≥n (m√°x 10 palabras)",
      "contexto": "CONTEXTO DETALLADO (m√≠nimo 100 palabras): Situaci√≥n que presenta m√∫ltiples alternativas a comparar con antecedentes de cada una, mercado/industria donde se aplican, casos de √©xito y fracaso previos, tendencias actuales, necesidades espec√≠ficas del caso de uso.",
      "descripcion": "DESCRIPCI√ìN EXHAUSTIVA (m√≠nimo 150 palabras): Detalles completos de cada alternativa incluyendo caracter√≠sticas t√©cnicas espec√≠ficas, ventajas y desventajas documentadas, costos detallados (inicial, operativo, mantenimiento), requisitos de implementaci√≥n, curva de aprendizaje, soporte disponible, casos de uso recomendados, limitaciones conocidas.",
      "pregunta": "Compara cr√≠ticamente todas las alternativas presentadas utilizando los criterios especificados y recomienda la mejor opci√≥n",
      "elementos_comparar": {
        "alternativa_1": {"nombre": "Nombre de alternativa 1", "caracteristicas": ["Caracter√≠stica t√©cnica 1 con detalles", "Caracter√≠stica 2 con m√©tricas", "Ventaja competitiva espec√≠fica", "Limitaci√≥n identificada", "Caso de uso ideal"]},
        "alternativa_2": {"nombre": "Nombre de alternativa 2", "caracteristicas": ["Caracter√≠stica t√©cnica 1 con detalles", "Caracter√≠stica 2 con m√©tricas", "Ventaja competitiva espec√≠fica", "Limitaci√≥n identificada", "Caso de uso ideal"]},
        "alternativa_3": {"nombre": "Nombre de alternativa 3", "caracteristicas": ["Caracter√≠stica t√©cnica 1 con detalles", "Caracter√≠stica 2 con m√©tricas", "Ventaja competitiva espec√≠fica", "Limitaci√≥n identificada", "Caso de uso ideal"]}
      },
      "criterios_comparacion": ["Rendimiento medible en benchmarks", "Costo total de propiedad (TCO)", "Facilidad de uso y curva de aprendizaje", "Escalabilidad y capacidad de crecimiento", "Soporte y ecosistema disponible", "Madurez y estabilidad de la soluci√≥n"],
      "respuesta_esperada": "Comparaci√≥n detallada (m√≠nimo 150 palabras) con matriz comparativa, evaluaci√≥n cr√≠tica de cada alternativa contra cada criterio, recomendaci√≥n fundamentada y escenarios donde cada opci√≥n ser√≠a la mejor",
      "puntos": 10
    }""",
            
            "predictivo": """    {
      "tipo": "case_study",
      "subtipo": "predictivo",
      "titulo": "T√≠tulo del escenario predictivo (m√°x 10 palabras)",
      "contexto": "CONTEXTO DETALLADO (m√≠nimo 100 palabras): Situaci√≥n actual completa con datos hist√≥ricos de los √∫ltimos meses/a√±os, tendencias observadas con gr√°ficas impl√≠citas, m√©tricas actuales con valores espec√≠ficos, indicadores clave de rendimiento, comparativas con per√≠odos anteriores, factores externos que est√°n influyendo.",
      "descripcion": "DESCRIPCI√ìN EXHAUSTIVA (m√≠nimo 150 palabras): Datos actuales muy espec√≠ficos incluyendo m√©tricas cuantitativas con n√∫meros exactos, tasas de crecimiento/decrecimiento observadas, patrones identificados en series de tiempo, eventos recientes que han impactado, se√±ales del mercado, comportamiento de la competencia, cambios en regulaciones, tendencias macroecon√≥micas, feedback de clientes/usuarios con estad√≠sticas.",
      "pregunta": "Bas√°ndote en los datos actuales y tendencias, proyecta el comportamiento futuro de esta situaci√≥n en los pr√≥ximos 6-12 meses",
      "datos_actuales": ["M√©trica 1: valor actual con % de cambio vs mes anterior", "Indicador 2: cifra espec√≠fica con tendencia observada", "KPI 3: n√∫mero actual con hist√≥rico de 3 meses", "Variable 4: dato cuantitativo con proyecci√≥n lineal", "Factor externo 5: impacto medido en la m√©trica principal"],
      "factores_considerar": ["Factor econ√≥mico externo con impacto estimado", "Variable de mercado con probabilidad de cambio", "Riesgo identificado con nivel de severidad", "Oportunidad potencial con timeframe", "Tendencia tecnol√≥gica con adopci√≥n esperada", "Cambio regulatorio con fecha de implementaci√≥n"],
      "respuesta_esperada": "Predicci√≥n fundamentada (m√≠nimo 150 palabras) con proyecciones num√©ricas espec√≠ficas, evidencia estad√≠stica que soporta la predicci√≥n, an√°lisis de tendencias, escenarios alternativos (optimista/realista/pesimista) y justificaci√≥n matem√°tica o l√≥gica del pron√≥stico",
      "puntos": 10
    }""",
            
            "simulacion": """    {
      "tipo": "case_study",
      "subtipo": "simulacion",
      "titulo": "T√≠tulo de la simulaci√≥n (m√°x 10 palabras)",
      "contexto": "CONTEXTO DETALLADO (m√≠nimo 100 palabras): Descripci√≥n completa del mundo simulado incluyendo reglas del sistema, objetivo general del simulacro, condiciones iniciales del escenario, recursos disponibles al inicio, restricciones del entorno, mec√°nicas de funcionamiento, c√≥mo interact√∫an las variables entre s√≠.",
      "descripcion": "DESCRIPCI√ìN EXHAUSTIVA (m√≠nimo 150 palabras): Descripci√≥n detallada del sistema completo incluyendo todas las reglas de operaci√≥n, c√≥mo cada variable afecta a las dem√°s (interdependencias), qu√© acciones est√°n permitidas, cu√°les son los l√≠mites del sistema, qu√© eventos aleatorios pueden ocurrir, c√≥mo se mide el √©xito, consecuencias de buenas y malas decisiones.",
      "pregunta": "Toma una secuencia de decisiones estrat√©gicas en este escenario simulado, justificando cada elecci√≥n bas√°ndote en el estado actual del sistema",
      "variables_dinamicas": {
        "variable_1": {"nombre": "Nombre descriptivo de variable 1", "valor_inicial": 100, "rango": "0-200", "descripcion": "Qu√© representa esta variable y c√≥mo impacta el sistema"},
        "variable_2": {"nombre": "Nombre de variable 2", "valor_inicial": "medio", "opciones": ["bajo", "medio", "alto"], "descripcion": "Significado de cada nivel y sus efectos"},
        "variable_3": {"nombre": "Variable 3", "valor_inicial": 50, "rango": "0-100", "descripcion": "Relaci√≥n con otras variables y thresholds cr√≠ticos"}
      },
      "decisiones_tomar": ["Decisi√≥n 1: descripci√≥n detallada de qu√© hay que decidir y qu√© opciones existen", "Decisi√≥n 2: contexto espec√≠fico y timing de esta decisi√≥n", "Decisi√≥n 3: trade-offs involucrados y consecuencias esperadas", "Decisi√≥n 4: dependencias con decisiones anteriores"],
      "respuesta_esperada": "Secuencia de decisiones (m√≠nimo 150 palabras) con justificaci√≥n detallada basada en el estado simulado del sistema, an√°lisis de c√≥mo cada decisi√≥n afecta las variables, predicci√≥n de outcomes, y estrategia global coherente",
      "puntos": 10
    }""",
            
            "inverso": """    {
      "tipo": "case_study",
      "subtipo": "inverso",
      "titulo": "T√≠tulo del caso inverso (m√°x 10 palabras)",
      "contexto": "CONTEXTO DETALLADO (m√≠nimo 100 palabras): Resultado final conocido con descripci√≥n completa del outcome obtenido, cu√°ndo ocurri√≥, qui√©nes estuvieron involucrados, qu√© se logr√≥ exactamente con m√©tricas espec√≠ficas, estado inicial conocido vs estado final, informaci√≥n disponible sobre el proceso.",
      "descripcion": "DESCRIPCI√ìN EXHAUSTIVA (m√≠nimo 150 palabras): Descripci√≥n muy detallada del resultado final incluyendo todas las caracter√≠sticas observables, m√©tricas de √©xito alcanzadas con n√∫meros espec√≠ficos, evidencias documentadas del outcome, comparaci√≥n con objetivos iniciales, impacto medible del resultado, testimonios o feedback disponible, documentaci√≥n existente del resultado.",
      "pregunta": "Trabajando hacia atr√°s desde el resultado final conocido, reconstruye el proceso l√≥gico que probablemente se sigui√≥ para llegar a este outcome",
      "resultado_final": "DESCRIPCI√ìN DETALLADA (m√≠nimo 80 palabras) del outcome final alcanzado con todas las m√©tricas, caracter√≠sticas, atributos, impactos medibles, evidencias concretas del √©xito o fracaso del resultado",
      "pistas": ["Pista 1: evidencia espec√≠fica encontrada con detalles", "Pista 2: dato conocido del proceso con contexto", "Pista 3: testimonio o documento que revela informaci√≥n", "Pista 4: patr√≥n observado en el resultado", "Pista 5: inconsistencia o anomal√≠a que da informaci√≥n"],
      "pasos_reconstruir": 5,
      "respuesta_esperada": "Reconstrucci√≥n l√≥gica (m√≠nimo 150 palabras) del proceso paso a paso con justificaci√≥n detallada de por qu√© cada paso fue necesario, evidencia que soporta cada etapa deducida, razonamiento l√≥gico de la secuencia, y validaci√≥n de que el proceso reconstruido llevar√≠a al resultado observado",
      "puntos": 10
    }""",
            
            "fallo": """    {
      "tipo": "case_study",
      "subtipo": "fallo",
      "titulo": "T√≠tulo del desastre/fallo (m√°x 10 palabras)",
      "contexto": "CONTEXTO DETALLADO (m√≠nimo 100 palabras): Situaci√≥n previa al fallo con antecedentes de qu√© se estaba intentando lograr, qui√©nes estaban involucrados, objetivos iniciales con m√©tricas esperadas, recursos invertidos, expectativas del mercado/stakeholders, presiones existentes, timeline del proyecto/iniciativa.",
      "descripcion": "DESCRIPCI√ìN EXHAUSTIVA (m√≠nimo 150 palabras): Descripci√≥n completa del fallo incluyendo cronolog√≠a detallada de eventos que llevaron al desastre, qu√© espec√≠ficamente sali√≥ mal con evidencia concreta, cu√°ndo se manifest√≥ el problema por primera vez, c√≥mo escal√≥, qu√© intentos de correcci√≥n fallaron, magnitud del fracaso con n√∫meros (p√©rdidas, impacto), reacciones de stakeholders, cobertura medi√°tica si aplica.",
      "pregunta": "Analiza en profundidad qu√© caus√≥ este fallo, identifica las se√±ales de alerta ignoradas y prop√≥n c√≥mo se pudo haber prevenido o mitigado",
      "se√±ales_alerta": ["Se√±al de alerta 1 ignorada: descripci√≥n espec√≠fica de qu√© warning se pas√≥ por alto y cu√°ndo", "Warning sign 2: indicador espec√≠fico que mostr√≥ problemas con timeline", "Red flag 3: evidencia de riesgo que no se atendi√≥ con consecuencias", "Alerta temprana 4: persona o sistema que alert√≥ sin ser escuchado"],
      "consecuencias": ["Consecuencia 1: impacto espec√≠fico con m√©trica cuantificada (ej: p√©rdida de $X millones)", "Impacto 2: efecto en stakeholders con descripci√≥n detallada", "P√©rdida 3: activo o recurso perdido con valoraci√≥n", "Da√±o reputacional 4: impacto en imagen con evidencia", "Consecuencia legal 5: demandas o sanciones enfrentadas"],
      "respuesta_esperada": "An√°lisis exhaustivo (m√≠nimo 150 palabras) de causas ra√≠z del fallo con evidencia espec√≠fica, identificaci√≥n de errores de proceso/juicio, lecciones aprendidas documentadas, medidas de prevenci√≥n concretas para evitar fallos similares en el futuro, y checklist de early warnings",
      "puntos": 10
    }""",
            
            "creativo": """    {
      "tipo": "case_study",
      "subtipo": "creativo",
      "titulo": "T√≠tulo del reto creativo (m√°x 10 palabras)",
      "contexto": "CONTEXTO DETALLADO (m√≠nimo 100 palabras): Situaci√≥n completa que requiere innovaci√≥n incluyendo estado actual del mercado/industria, necesidad identificada con evidencia de demanda, soluciones existentes y sus limitaciones, oportunidad de innovaci√≥n con tama√±o de mercado potencial, tendencias que favorecen la innovaci√≥n, casos inspiradores de innovaciones similares.",
      "descripcion": "DESCRIPCI√ìN EXHAUSTIVA (m√≠nimo 150 palabras): Descripci√≥n detallada del desaf√≠o u oportunidad de innovaci√≥n incluyendo pain points espec√≠ficos de usuarios/clientes con evidencia cualitativa, gaps en soluciones actuales con ejemplos concretos, restricciones tecnol√≥gicas o de mercado, barreras de entrada, ventana de oportunidad, stakeholders que se beneficiar√≠an, recursos disponibles para innovar.",
      "pregunta": "Prop√≥n una soluci√≥n creativa, original e innovadora para este desaf√≠o que sea viable y tenga alto potencial de impacto",
      "restricciones": ["Restricci√≥n presupuestaria: m√°ximo $X para desarrollo inicial", "Limitaci√≥n tecnol√≥gica: debe ser compatible con Y", "Constraint de tiempo: lanzamiento en Z meses", "Restricci√≥n regulatoria: debe cumplir con norma ABC", "Limitaci√≥n de recursos: equipo de N personas disponible"],
      "criterios_creatividad": ["Originalidad: qu√© tan novedosa es la idea vs soluciones existentes", "Viabilidad t√©cnica: puede construirse con tecnolog√≠a actual", "Impacto potencial: tama√±o del problema resuelto y beneficio generado", "Innovaci√≥n disruptiva: cambia paradigmas o crea nuevos mercados", "Escalabilidad: puede crecer sin multiplicar costos linealmente", "User experience: qu√© tan intuitiva y deseabl e es la soluci√≥n"],
      "respuesta_esperada": "Idea innovadora (m√≠nimo 150 palabras) con descripci√≥n detallada de c√≥mo funciona, qu√© problema resuelve espec√≠ficamente, por qu√© es diferente de lo existente, justificaci√≥n de viabilidad t√©cnica, an√°lisis de mercado potencial, mockups o ejemplos conceptuales, y plan de validaci√≥n de la idea",
      "puntos": 10
    }""",
            
            "etico": """    {
      "tipo": "case_study",
      "subtipo": "etico",
      "titulo": "T√≠tulo del dilema √©tico (m√°x 10 palabras)",
      "contexto": "CONTEXTO DETALLADO (m√≠nimo 100 palabras): Situaci√≥n completa donde surge el conflicto √©tico incluyendo antecedentes de la empresa/organizaci√≥n, presiones de negocio espec√≠ficas, stakeholders involucrados con sus intereses, marco regulatorio aplicable, precedentes de casos similares, cultura organizacional, valores declarados vs pr√°cticas reales.",
      "descripcion": "DESCRIPCI√ìN EXHAUSTIVA (m√≠nimo 150 palabras): Descripci√≥n completa del conflicto entre beneficio empresarial y consideraciones √©ticas/morales incluyendo n√∫meros espec√≠ficos de impacto financiero, qui√©nes se benefician vs qui√©nes se perjudican, consecuencias legales potenciales, impacto reputacional, presi√≥n de competidores, expectativas de accionistas, opini√≥n p√∫blica, evidencia de casos similares.",
      "pregunta": "¬øQu√© decisi√≥n es la correcta balanceando perspectivas √©ticas, legales y empresariales? Justifica tu posici√≥n considerando todos los stakeholders",
      "stakeholders": ["Stakeholder 1: descripci√≥n del grupo con sus intereses espec√≠ficos y poder de influencia", "Parte interesada 2: qui√©nes son y qu√© ganan/pierden en cada escenario", "Grupo afectado 3: impacto directo en su bienestar con detalle", "Stakeholder 4: expectativas y poder de presi√≥n que ejercen", "Parte 5: derechos que est√°n en juego"],
      "dilema": "DILEMA ESPEC√çFICO (m√≠nimo 50 palabras): Descripci√≥n detallada del conflicto √©tico espec√≠fico con las dos o m√°s opciones mutuamente excluyentes, qu√© valores entran en conflicto, qu√© principios √©ticos est√°n en tensi√≥n, ejemplos concretos del trade-off",
      "consideraciones_eticas": ["Principio √©tico 1: explicaci√≥n de qu√© norma moral aplica y c√≥mo", "Valor 2: qu√© valor fundamental est√° en juego con justificaci√≥n", "Norma moral 3: est√°ndar √©tico relevante de la industria o sociedad", "Framework √©tico 4: lente de an√°lisis (utilitarismo, deontolog√≠a, √©tica de virtudes)", "Precedente 5: casos hist√≥ricos similares y sus resoluciones"],
      "respuesta_esperada": "Decisi√≥n fundamentada (m√≠nimo 150 palabras) balanceando √©tica, legalidad y necesidades del negocio con an√°lisis de consecuencias de cada opci√≥n, framework √©tico aplicado, justificaci√≥n moral robusta, consideraci√≥n de todos los stakeholders, y plan de implementaci√≥n que minimice da√±os",
      "puntos": 10
    }""",
            
            "tecnico": """    {
      "tipo": "case_study",
      "subtipo": "tecnico",
      "titulo": "T√≠tulo del caso t√©cnico (m√°x 10 palabras)",
      "contexto": "CONTEXTO DETALLADO (m√≠nimo 100 palabras): Descripci√≥n completa del sistema, algoritmo o proceso t√©cnico actual incluyendo arquitectura general, tecnolog√≠as utilizadas, escala de operaci√≥n, volumetr√≠a de datos procesados, usuarios concurrentes, infraestructura donde corre, historial de performance, evoluci√≥n del sistema en el tiempo.",
      "descripcion": "DESCRIPCI√ìN EXHAUSTIVA (m√≠nimo 150 palabras): Descripci√≥n t√©cnica completa del sistema incluyendo componentes espec√≠ficos con sus versiones, flujo de datos detallado, integraciones existentes, m√©tricas de rendimiento actuales con n√∫meros exactos (latencia P95, throughput, error rate), cuellos de botella identificados con evidencia de profiling, limitaciones de la arquitectura actual, deuda t√©cnica acumulada, incidentes recientes relacionados con performance.",
      "pregunta": "Optimiza este sistema t√©cnico mejorando significativamente su rendimiento mientras mantienes o reduces costos operativos",
      "metricas_actuales": {
        "rendimiento": "Descripci√≥n de m√©trica con valor actual espec√≠fico (ej: 'Latencia P95: 450ms, objetivo <200ms')",
        "throughput": "Capacidad actual con unidades (ej: '1,500 req/seg, picos de 2,000')",
        "eficiencia": "Uso de recursos con porcentajes (ej: 'CPU al 75% promedio, RAM 80% utilizada')",
        "costo": "Costo operativo mensual (ej: '$12,000/mes en infra cloud')",
        "disponibilidad": "SLA actual y uptime (ej: '99.5% uptime, objetivo 99.9%')"
      },
      "limitaciones_tecnicas": ["Limitaci√≥n 1: constraint espec√≠fico con impacto medible (ej: 'DB single-threaded limita writes a 5k/seg')", "Constraint t√©cnico 2: bottleneck identificado con evidencia", "Limitaci√≥n 3: dependencia legacy que genera fricci√≥n", "Constraint 4: restricci√≥n de infraestructura o presupuesto", "Limitaci√≥n 5: compatibilidad requerida que limita opciones"],
      "objetivos_optimizacion": ["Mejorar latencia P95 en 50% (de 450ms a <225ms)", "Incrementar throughput en 3x (de 1.5k a 4.5k req/seg)", "Reducir costos de infraestructura en 30% ($12k a $8.4k/mes)", "Alcanzar 99.9% uptime (actualmente 99.5%)", "Reducir error rate de 0.5% a <0.1%"],
      "respuesta_esperada": "Propuesta de optimizaci√≥n t√©cnica (m√≠nimo 150 palabras) con arquitectura mejorada detallada, cambios espec√≠ficos propuestos con justificaci√≥n t√©cnica, estimaci√≥n cuantitativa de mejoras esperadas en cada m√©trica, an√°lisis de costo-beneficio, plan de implementaci√≥n por fases, estrategia de testing y rollback, y m√©tricas para validar √©xito",
      "puntos": 10
    }"""
        }
        
        return formatos_casos.get(tipo_caso, formatos_casos["descriptivo"])
    
    def _extraer_preguntas(self, respuesta: str, num_preguntas: Dict[str, int] = None) -> List[PreguntaExamen]:
        """Extrae preguntas del JSON"""
        try:
            print(f"\n{'='*60}")
            print(f"üîç EXTRAYENDO JSON DE LA RESPUESTA")
            print(f"{'='*60}")
            
            # Estrategia mejorada para DeepSeek-R1: buscar TODOS los bloques JSON potenciales
            # Buscar todas las apariciones de { que puedan ser inicio de JSON
            posibles_jsons = []
            
            for i, char in enumerate(respuesta):
                if char == '{':
                    # Encontrar el cierre balanceado del JSON desde esta posici√≥n
                    nivel = 0
                    fin = i
                    en_string = False
                    escape = False
                    
                    for j in range(i, len(respuesta)):
                        c = respuesta[j]
                        
                        if escape:
                            escape = False
                            continue
                        
                        if c == '\\':
                            escape = True
                            continue
                        
                        if c == '"':
                            en_string = not en_string
                            continue
                        
                        if not en_string:
                            if c == '{':
                                nivel += 1
                            elif c == '}':
                                nivel -= 1
                                if nivel == 0:
                                    fin = j + 1
                                    json_candidato = respuesta[i:fin]
                                    # Solo considerar JSONs que parezcan razonables (> 50 chars)
                                    if len(json_candidato) > 50:
                                        posibles_jsons.append((i, fin, json_candidato))
                                    break
            
            print(f"üîç Encontrados {len(posibles_jsons)} bloques JSON potenciales")
            
            # Estrategia 1: Buscar JSON completo balanceado
            json_str = None
            inicio = -1
            fin = -1
            
            # Intentar parsear cada JSON candidato, priorizando los m√°s largos
            posibles_jsons.sort(key=lambda x: len(x[2]), reverse=True)
            
            # FASE 1: Buscar JSON con array de preguntas completo
            for idx, (start, end, candidato) in enumerate(posibles_jsons):
                print(f"  üì¶ Candidato {idx+1}: posici√≥n {start}-{end}, tama√±o {len(candidato)} chars")
                print(f"     Inicio: {candidato[:80]}...")
                
                # PRIORIDAD 1: JSON con "preguntas" o "questions" (array completo)
                if '"preguntas"' in candidato or '"questions"' in candidato:
                    # RECHAZAR si contiene placeholders COMO VALORES (no en texto de preguntas)
                    tiene_placeholders = False
                    if ('"puntos": ...' in candidato or 
                        '"puntos":...' in candidato or
                        '"opciones": [...]' in candidato or
                        '"pregunta": "..."' in candidato or
                        '": "..."' in candidato):
                        print(f"     ‚ö†Ô∏è Contiene placeholders COMO VALORES, descartando")
                        tiene_placeholders = True
                    
                    if not tiene_placeholders:
                        print(f"     ‚úÖ Contiene array de preguntas y NO tiene placeholders")
                        json_str = candidato
                        inicio = start
                        fin = end
                        break
                else:
                    print(f"     ‚è≠Ô∏è No contiene array 'preguntas', continuando b√∫squeda...")
            
            # FASE 2: Si no encontr√≥ array completo, buscar preguntas individuales
            if json_str is None:
                print(f"  üí° No se encontr√≥ array completo, buscando preguntas individuales...")
                for idx, (start, end, candidato) in enumerate(posibles_jsons):
                    if '"tipo"' in candidato or '"type"' in candidato:
                        tiene_placeholders = False
                        if ('"puntos": ...' in candidato or 
                            '"puntos":...' in candidato or
                            '"opciones": [...]' in candidato or
                            '"pregunta": "..."' in candidato or
                            '": "..."' in candidato):
                            continue
                        
                        print(f"     ‚úÖ Usando pregunta individual como fallback")
                        json_str = candidato
                        inicio = start
                        fin = end
                        break
            
            if json_str is None and posibles_jsons:
                # Si ninguno tiene campos de pregunta, tomar el m√°s largo
                inicio, fin, json_str = posibles_jsons[0]
                print(f"  üí° Usando JSON m√°s largo como fallback")
            
            if json_str:
                print(f"‚úÖ JSON seleccionado en posici√≥n {inicio}-{fin}")
                print(f"üìÑ JSON extra√≠do (primeros 300 chars):\n{json_str[:300]}...\n")
                
                # Limpiar JSON de errores comunes del modelo
                import re
                
                # 1. Eliminar comas antes de ] o }
                json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)
                
                # 2. Si el JSON termina incompleto, intentar repararlo
                # Buscar campos incompletos tipo: "explanation": "
                json_str = re.sub(r'"\s*:\s*"[^"]*$', '": "Explicaci√≥n no disponible"', json_str)
                
                # 3. Asegurar cierre de objetos y arrays
                # Contar llaves y corchetes abiertos
                count_braces = json_str.count('{') - json_str.count('}')
                count_brackets = json_str.count('[') - json_str.count(']')
                
                # Cerrar los que falten
                if count_braces > 0 or count_brackets > 0:
                    print(f"‚ö†Ô∏è JSON incompleto: {count_braces} llaves, {count_brackets} corchetes sin cerrar")
                    # Cerrar en orden inverso (primero objetos, luego arrays)
                    json_str += '}' * count_braces
                    json_str += ']' * count_brackets
                    print(f"üîß JSON reparado autom√°ticamente")
                
                # Intentar parsear
                try:
                    datos = json.loads(json_str)
                    print(f"‚úÖ JSON parseado correctamente")
                    
                    # Registrar JSON extra√≠do
                    self._agregar_log('json_extraido', json_str)
                    
                    # Verificar estructura - aceptar 'preguntas' o 'questions'
                    campo_preguntas = None
                    if 'preguntas' in datos:
                        campo_preguntas = 'preguntas'
                    elif 'questions' in datos:
                        campo_preguntas = 'questions'
                    
                    if not campo_preguntas:
                        print(f"‚ùå JSON no tiene campo 'preguntas' ni 'questions'")
                        print(f"üìã Campos encontrados: {list(datos.keys())}")
                        
                        # Si el JSON es un array directamente, usarlo
                        if isinstance(datos, list):
                            print(f"üí° El JSON es un array directo con {len(datos)} elementos")
                            campo_preguntas = None
                            lista_preguntas = datos
                        # Si es un objeto con campos de pregunta (type, statement, etc.), es UNA pregunta
                        elif 'type' in datos or 'tipo' in datos:
                            print(f"üí° El JSON es una pregunta √∫nica, convirti√©ndola a array")
                            lista_preguntas = [datos]
                        else:
                            print(f"‚ö†Ô∏è JSON no reconocido, retornando vac√≠o")
                            return []
                    else:
                        lista_preguntas = datos.get(campo_preguntas, [])
                    
                    preguntas = []
                    preguntas_parseadas_log = []
                    for i, p in enumerate(lista_preguntas):
                        try:
                            pregunta = PreguntaExamen.from_dict(p)
                            preguntas.append(pregunta)
                            preguntas_parseadas_log.append(pregunta.to_dict() if hasattr(pregunta, 'to_dict') else str(pregunta))
                            tipo = p.get('tipo') or p.get('type', 'unknown')
                            texto = p.get('pregunta') or p.get('question', '')
                            print(f"‚úÖ Pregunta {i+1}: {tipo} - {texto[:50]}...")
                        except Exception as e:
                            error_msg = f"Error en pregunta {i+1}: {e}"
                            print(f"‚ùå {error_msg}")
                            self._agregar_log('errores', error_msg)
                            continue
                    
                    # Registrar preguntas parseadas
                    self._agregar_log('preguntas_parseadas', preguntas_parseadas_log)
                    
                    print(f"\n‚úÖ Total: {len(preguntas)} preguntas generadas exitosamente")
                    
                    # FILTRAR PREGUNTAS POR TIPO Y CANTIDAD SOLICITADA
                    if num_preguntas and any(v > 0 for v in num_preguntas.values()):
                        preguntas_filtradas = []
                        contador_por_tipo = {}
                        
                        # Mapeo de tipos nuevos a tipos del sistema
                        mapeo_tipos = {
                            'flashcard': 'flashcard',
                            'mcq': 'mcq', 
                            'true_false': 'true_false',
                            'verdadero_falso': 'true_false',
                            'cloze': 'cloze',
                            'short_answer': 'short_answer',
                            'respuesta_corta': 'short_answer',
                            'open_question': 'open_question',
                            'desarrollo': 'open_question',
                            'case_study': 'case_study',
                            'caso_estudio': 'case_study',
                            'reading_comprehension': 'reading_comprehension',
                            'reading_true_false': 'reading_true_false',
                            'reading_cloze': 'reading_cloze',
                            'reading_skill': 'reading_skill',
                            'reading_matching': 'reading_matching',
                            'reading_sequence': 'reading_sequence',
                            'writing_short': 'writing_short',
                            'writing_paraphrase': 'writing_paraphrase',
                            'writing_correction': 'writing_correction',
                            'writing_transformation': 'writing_transformation',
                            'writing_essay': 'writing_essay',
                            'writing_sentence_builder': 'writing_sentence_builder',
                            'writing_picture_description': 'writing_picture_description',
                            'writing_email': 'writing_email',
                            'multiple': 'mcq',
                            'corta': 'short_answer'
                        }
                        
                        for pregunta in preguntas:
                            # DEBUG: Imprimir tipo exacto de la pregunta
                            print(f"  üîç Pregunta tipo='{pregunta.tipo}' (repr: {repr(pregunta.tipo)})")
                            tipo_normalizado = mapeo_tipos.get(pregunta.tipo, pregunta.tipo)
                            print(f"     ‚Üí Normalizado a: '{tipo_normalizado}'")
                            cantidad_solicitada = num_preguntas.get(tipo_normalizado, 0)
                            print(f"     ‚Üí Cantidad solicitada de '{tipo_normalizado}': {cantidad_solicitada}")
                            
                            if cantidad_solicitada > 0:
                                if tipo_normalizado not in contador_por_tipo:
                                    contador_por_tipo[tipo_normalizado] = 0
                                
                                if contador_por_tipo[tipo_normalizado] < cantidad_solicitada:
                                    preguntas_filtradas.append(pregunta)
                                    contador_por_tipo[tipo_normalizado] += 1
                        
                        print(f"üîç Filtrado: {len(preguntas)} generadas ‚Üí {len(preguntas_filtradas)} solicitadas")
                        print(f"   Solicitadas: {num_preguntas}")
                        print(f"   Filtradas por tipo: {contador_por_tipo}")
                        
                        # Registrar filtrado
                        self._agregar_log('filtrado', {
                            'total_generadas': len(preguntas),
                            'total_filtradas': len(preguntas_filtradas),
                            'solicitadas': num_preguntas,
                            'contador_por_tipo': contador_por_tipo
                        })
                        
                        # Verificar si faltaron preguntas
                        tipos_faltantes = []
                        for tipo, cantidad in num_preguntas.items():
                            generadas = contador_por_tipo.get(tipo, 0)
                            if generadas < cantidad:
                                tipos_faltantes.append(f"{tipo}: {generadas}/{cantidad}")
                        
                        if tipos_faltantes:
                            warning_msg = f"El modelo no gener√≥ suficientes preguntas: {', '.join(tipos_faltantes)}"
                            self._agregar_log('errores', warning_msg)
                            print(f"‚ö†Ô∏è ADVERTENCIA: El modelo no gener√≥ suficientes preguntas de algunos tipos:")
                            for faltante in tipos_faltantes:
                                print(f"   - {faltante}")
                            print(f"   Esto puede ocurrir porque:")
                            print(f"   1. El modelo gener√≥ tipos diferentes a los solicitados")
                            print(f"   2. El modelo ignor√≥ las instrucciones del prompt")
                            print(f"   3. El contenido es muy corto para generar m√°s preguntas")
                            print(f"   üí° Intenta regenerar la pr√°ctica")
                        
                        # Registrar resultado final
                        resultado_final = [p.to_dict() if hasattr(p, 'to_dict') else str(p) for p in preguntas_filtradas]
                        self._agregar_log('resultado_final', resultado_final)
                        self._guardar_log()
                        
                        return preguntas_filtradas
                    
                    # Si no hay filtrado, retornar todas las preguntas
                    resultado_final = [p.to_dict() if hasattr(p, 'to_dict') else str(p) for p in preguntas]
                    self._agregar_log('resultado_final', resultado_final)
                    self._guardar_log()
                    
                    return preguntas
                    
                except json.JSONDecodeError as e:
                        error_msg = f"Error parseando JSON: {e}"
                        print(f"‚ùå {error_msg}")
                        print(f"üìÑ JSON problem√°tico (primeros 500):\n{json_str[:500]}")
                        print(f"üìÑ √öltimos 200 caracteres:\n{json_str[-200:]}")
                        
                        # INTENTO DE REPARACI√ìN AGRESIVA
                        print(f"\nüîß Intentando reparaci√≥n agresiva del JSON...")
                        
                        # Eliminar texto despu√©s del √∫ltimo } v√°lido
                        ultimo_cierre = json_str.rfind('}')
                        if ultimo_cierre > 0:
                            json_str_cortado = json_str[:ultimo_cierre + 1]
                            
                            # Asegurar que cierra el array de preguntas
                            if '"preguntas"' in json_str_cortado and not json_str_cortado.strip().endswith(']}'):
                                json_str_cortado = json_str_cortado.rstrip() + ']}'
                            
                            try:
                                datos = json.loads(json_str_cortado)
                                print(f"‚úÖ JSON reparado exitosamente cortando al √∫ltimo }}")
                                
                                # Continuar con el procesamiento normal
                                campo_preguntas = None
                                if 'preguntas' in datos:
                                    campo_preguntas = 'preguntas'
                                elif 'questions' in datos:
                                    campo_preguntas = 'questions'
                                
                                if campo_preguntas:
                                    lista_preguntas = datos[campo_preguntas]
                                    if isinstance(lista_preguntas, list):
                                        # Convertir a objetos PreguntaExamen
                                        preguntas = []
                                        preguntas_parseadas_log = []
                                        
                                        for i, pregunta_dict in enumerate(lista_preguntas):
                                            try:
                                                pregunta_obj = PreguntaExamen.from_dict(pregunta_dict)
                                                preguntas.append(pregunta_obj)
                                                print(f"‚úÖ Pregunta {i+1}: {pregunta_obj.tipo} - {pregunta_obj.pregunta[:50]}...")
                                                preguntas_parseadas_log.append(pregunta_obj.to_dict())
                                            except Exception as e:
                                                error_msg = f"Error en pregunta {i+1}: {e}"
                                                print(f"‚ùå {error_msg}")
                                                self._agregar_log('errores', error_msg)
                                                continue
                                        
                                        self._agregar_log('preguntas_parseadas', preguntas_parseadas_log)
                                        print(f"\n‚úÖ Total: {len(preguntas)} preguntas generadas exitosamente")
                                        
                                        # Aplicar filtrado si es necesario
                                        if num_preguntas and any(v > 0 for v in num_preguntas.values()):
                                            preguntas = self._filtrar_preguntas(preguntas, num_preguntas)
                                        
                                        resultado_final = [p.to_dict() if hasattr(p, 'to_dict') else str(p) for p in preguntas]
                                        self._agregar_log('resultado_final', resultado_final)
                                        self._guardar_log()
                                        return preguntas
                            except:
                                pass
                        
                        self._agregar_log('errores', error_msg)
                        self._agregar_log('json_extraido', json_str[:1000])
                        self._guardar_log()
                        return []
            
            # Estrategia 2: Buscar bloques de c√≥digo markdown
            print("‚ö†Ô∏è No se encontr√≥ JSON directo, buscando en bloques markdown...")
            if "```json" in respuesta:
                print("üí° Se detect√≥ bloque ```json```, intentando extraer...")
                inicio_markdown = respuesta.find("```json") + 7
                fin_markdown = respuesta.find("```", inicio_markdown)
                if fin_markdown > inicio_markdown:
                    json_str = respuesta[inicio_markdown:fin_markdown].strip()
                    print(f"‚úÖ JSON encontrado en markdown")
                    try:
                        datos = json.loads(json_str)
                        # Aceptar 'preguntas' o 'questions'
                        lista_preguntas = datos.get('preguntas') or datos.get('questions', [])
                        if isinstance(datos, list):
                            lista_preguntas = datos
                        
                        preguntas = []
                        for p in lista_preguntas:
                            preguntas.append(PreguntaExamen.from_dict(p))
                        print(f"‚úÖ {len(preguntas)} preguntas desde markdown")
                        return preguntas
                    except Exception as e:
                        print(f"‚ùå Error parseando markdown JSON: {e}")
            
            print("‚ùå No se pudo extraer JSON v√°lido de la respuesta")
            error_msg = "No se encontr√≥ JSON v√°lido en la respuesta del modelo"
            self._agregar_log('errores', error_msg)
            self._guardar_log()
            return []
            
        except Exception as e:
            error_msg = f"Error general extrayendo JSON: {e}"
            print(f"‚ùå {error_msg}")
            import traceback
            traceback.print_exc()
            self._agregar_log('errores', error_msg)
            self._guardar_log()
            return []
    
    def evaluar_respuesta(self, pregunta: PreguntaExamen, respuesta_usuario) -> dict:
        """Eval√∫a una respuesta del usuario"""
        resultado = {
            "correcta": False,
            "puntos_obtenidos": 0,
            "feedback": ""
        }
        
        # Convertir lista a string si es necesario
        if isinstance(respuesta_usuario, list):
            respuesta_usuario = respuesta_usuario[0] if respuesta_usuario else ""
        
        # Validar que respuesta_usuario sea string
        if not isinstance(respuesta_usuario, str):
            respuesta_usuario = str(respuesta_usuario)
        
        if not respuesta_usuario or respuesta_usuario.strip() == "":
            resultado["feedback"] = "No se proporcion√≥ respuesta"
            return resultado
        
        respuesta_usuario_lower = respuesta_usuario.strip().lower()
        respuesta_correcta = pregunta.respuesta_correcta
        if respuesta_correcta is None:
            respuesta_correcta = ""
        elif not isinstance(respuesta_correcta, str):
            respuesta_correcta = str(respuesta_correcta)
        respuesta_correcta_lower = respuesta_correcta.strip().lower()
        
        if pregunta.tipo == "multiple" or pregunta.tipo == "mcq":
            # Para m√∫ltiple, solo comparar la letra (A, B, C, D)
            if respuesta_usuario_lower in respuesta_correcta_lower or respuesta_correcta_lower in respuesta_usuario_lower:
                resultado["correcta"] = True
                resultado["puntos_obtenidos"] = pregunta.puntos
                resultado["feedback"] = "¬°Correcto!"
            else:
                resultado["feedback"] = f"Incorrecto. La respuesta correcta es: {pregunta.respuesta_correcta}"
        
        elif pregunta.tipo == "verdadero_falso" or pregunta.tipo == "true_false":
            # Extraer respuesta correcta de metadata si existe
            respuesta_correcta_display = pregunta.respuesta_correcta
            if hasattr(pregunta, 'metadata') and pregunta.metadata:
                if isinstance(pregunta.metadata, dict):
                    correct_answer = pregunta.metadata.get('correct_answer')
                    if correct_answer is not None:
                        respuesta_correcta_display = 'Verdadero' if correct_answer else 'Falso'
            
            if respuesta_usuario_lower == respuesta_correcta_lower:
                resultado["correcta"] = True
                resultado["puntos_obtenidos"] = pregunta.puntos
                resultado["feedback"] = "¬°Correcto!"
            else:
                resultado["feedback"] = f"Incorrecto. La respuesta correcta es: {respuesta_correcta_display}"
        
        elif pregunta.tipo in ["corta", "desarrollo", "short_answer", "open_question", "case_study",
                               "flashcard", "cloze",
                               "reading_comprehension", "reading_true_false", "reading_cloze", 
                               "reading_skill", "reading_matching", "reading_sequence",
                               "writing_short", "writing_paraphrase", "writing_correction",
                               "writing_transformation", "writing_essay", "writing_sentence_builder",
                               "writing_picture_description", "writing_email"]:
            # Para todos los dem√°s tipos, usar IA para evaluar
            print(f"\nü§ñ Evaluando respuesta de tipo '{pregunta.tipo}' con IA...")
            resultado = self._evaluar_con_ia(pregunta, respuesta_usuario)
        
        else:
            # Tipo desconocido - evaluar con IA por defecto
            print(f"\n‚ö†Ô∏è Tipo de pregunta desconocido: '{pregunta.tipo}' - usando evaluaci√≥n con IA")
            resultado = self._evaluar_con_ia(pregunta, respuesta_usuario)
        
        return resultado
    
    def _evaluar_con_ia(self, pregunta: PreguntaExamen, respuesta_usuario: str) -> dict:
        """Eval√∫a una respuesta de desarrollo/corta usando IA"""
        
        # Extraer respuesta correcta dependiendo del tipo
        respuesta_modelo = pregunta.respuesta_correcta
        
        # Para flashcards, extraer de metadata.solution.answer
        if pregunta.tipo == 'flashcard' and hasattr(pregunta, 'metadata') and pregunta.metadata:
            if isinstance(pregunta.metadata, dict):
                solution = pregunta.metadata.get('solution', {})
                if isinstance(solution, dict):
                    respuesta_modelo = solution.get('answer', respuesta_modelo)
        
        # Para casos de estudio, extraer de metadata.sample_answer
        elif pregunta.tipo == 'case_study' and hasattr(pregunta, 'metadata') and pregunta.metadata:
            if isinstance(pregunta.metadata, dict):
                respuesta_modelo = pregunta.metadata.get('sample_answer', respuesta_modelo)
        
        # Si es un diccionario (fallback), extraer el campo 'answer'
        if isinstance(respuesta_modelo, dict):
            respuesta_modelo = respuesta_modelo.get('answer', str(respuesta_modelo))
        
        # Convertir a string si no lo es
        if not isinstance(respuesta_modelo, str):
            respuesta_modelo = str(respuesta_modelo)
        
        # Si a√∫n es None o vac√≠o, usar un placeholder
        if not respuesta_modelo or respuesta_modelo == 'None':
            respuesta_modelo = "No hay respuesta modelo definida para esta pregunta"
        
        prompt = f"""Eres un profesor evaluando una respuesta de estudiante. Compara la respuesta del estudiante con la respuesta modelo y proporciona retroalimentaci√≥n espec√≠fica.

PREGUNTA:
{pregunta.pregunta}

RESPUESTA MODELO (lo que se esperaba):
{respuesta_modelo}

RESPUESTA DEL ESTUDIANTE:
{respuesta_usuario}

PUNTOS M√ÅXIMOS: {pregunta.puntos}

INSTRUCCIONES DE EVALUACI√ìN:
1. Identifica los CONCEPTOS CLAVE en la respuesta modelo
2. Verifica cu√°les de esos conceptos est√°n presentes en la respuesta del estudiante
3. Identifica qu√© conceptos FALTAN o est√°n INCOMPLETOS
4. Asigna puntos proporcionales a los conceptos presentes
5. Proporciona retroalimentaci√≥n ESPEC√çFICA sobre qu√© falta comprender

Responde √öNICAMENTE con JSON en este formato exacto:
{{
  "puntos": <n√∫mero decimal de 0 a {pregunta.puntos}>,
  "conceptos_correctos": ["concepto1", "concepto2"],
  "conceptos_faltantes": ["concepto3", "concepto4"],
  "feedback": "Retroalimentaci√≥n espec√≠fica explicando qu√© conceptos domina y cu√°les le faltan comprender"
}}

JSON:"""

        try:
            if self.usar_ollama:
                # Evaluar con Ollama
                response = requests.post(
                    "http://localhost:11434/api/generate",
                    json={
                        "model": self.modelo_ollama,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.3,  # M√°s determin√≠stico
                            "num_predict": 400   # M√°s tokens para retroalimentaci√≥n detallada
                        }
                    },
                    timeout=60
                )
                
                if response.status_code == 200:
                    respuesta_ia = response.json()['response']
                    print(f"üìù Respuesta IA (primeros 300 chars): {respuesta_ia[:300]}")
                    
                    # Extraer JSON balanceado
                    inicio = respuesta_ia.find('{')
                    if inicio >= 0:
                        nivel = 0
                        fin = inicio
                        en_string = False
                        escape = False
                        
                        for i in range(inicio, len(respuesta_ia)):
                            char = respuesta_ia[i]
                            
                            if escape:
                                escape = False
                                continue
                            
                            if char == '\\':
                                escape = True
                                continue
                            
                            if char == '"':
                                en_string = not en_string
                                continue
                            
                            if not en_string:
                                if char == '{':
                                    nivel += 1
                                elif char == '}':
                                    nivel -= 1
                                    if nivel == 0:
                                        fin = i + 1
                                        break
                        
                        if fin > inicio:
                            json_str = respuesta_ia[inicio:fin]
                            evaluacion = json.loads(json_str)
                            
                            puntos = float(evaluacion.get('puntos', 0))
                            conceptos_correctos = evaluacion.get('conceptos_correctos', [])
                            conceptos_faltantes = evaluacion.get('conceptos_faltantes', [])
                            feedback_base = evaluacion.get('feedback', 'Sin evaluaci√≥n')
                            
                            # Construir feedback detallado
                            feedback = feedback_base
                            if conceptos_correctos:
                                feedback += f"\\n\\n‚úÖ Conceptos que dominas: {', '.join(conceptos_correctos)}"
                            if conceptos_faltantes:
                                feedback += f"\\n\\n‚ùå Conceptos que te faltan comprender: {', '.join(conceptos_faltantes)}"
                            
                            print(f"‚úÖ Evaluaci√≥n: {puntos}/{pregunta.puntos} puntos")
                            print(f"‚úÖ Conceptos correctos: {conceptos_correctos}")
                            print(f"‚ùå Conceptos faltantes: {conceptos_faltantes}")
                            
                            return {
                                "correcta": puntos >= pregunta.puntos * 0.6,  # 60% o m√°s es correcto
                                "puntos_obtenidos": puntos,
                                "feedback": feedback,
                                "conceptos_correctos": conceptos_correctos,
                                "conceptos_faltantes": conceptos_faltantes
                            }
            
            # Fallback: evaluaci√≥n simple por palabras clave
            print("‚ö†Ô∏è Usando evaluaci√≥n fallback")
            
            # Extraer texto de respuesta correcta (puede ser dict en flashcards)
            respuesta_modelo = pregunta.respuesta_correcta
            if isinstance(respuesta_modelo, dict):
                respuesta_modelo = respuesta_modelo.get('answer', str(respuesta_modelo))
            if not isinstance(respuesta_modelo, str):
                respuesta_modelo = str(respuesta_modelo)
            
            palabras_correctas = set(respuesta_modelo.lower().split())
            palabras_usuario = set(respuesta_usuario.lower().split())
            coincidencias = len(palabras_correctas.intersection(palabras_usuario))
            similitud = coincidencias / len(palabras_correctas) if palabras_correctas else 0
            
            puntos = pregunta.puntos * similitud
            
            if similitud >= 0.7:
                feedback = "¬°Excelente! Respuesta muy completa."
            elif similitud >= 0.5:
                feedback = "Bien, pero podr√≠as agregar m√°s detalles."
            elif similitud >= 0.3:
                feedback = "Parcialmente correcto, faltan conceptos clave."
            else:
                feedback = f"Incompleto. Respuesta esperada: {respuesta_modelo}"
            
            return {
                "correcta": similitud >= 0.6,
                "puntos_obtenidos": round(puntos, 1),
                "feedback": feedback
            }
            
        except Exception as e:
            print(f"‚ùå Error evaluando con IA: {e}")
            import traceback
            traceback.print_exc()
            
            # Extraer respuesta modelo para fallback
            respuesta_modelo = pregunta.respuesta_correcta
            if isinstance(respuesta_modelo, dict):
                respuesta_modelo = respuesta_modelo.get('answer', 'No disponible')
            if not isinstance(respuesta_modelo, str):
                respuesta_modelo = str(respuesta_modelo)
            
            # Fallback
            return {
                "correcta": False,
                "puntos_obtenidos": 0,
                "feedback": f"Error en evaluaci√≥n. Respuesta esperada: {respuesta_modelo}"
            }


# Funci√≥n de utilidad para verificar qu√© usar
def detectar_backend_disponible():
    """Detecta qu√© backend est√° disponible"""
    # Verificar Ollama
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=2)
        if response.status_code == 200:
            modelos = response.json().get('models', [])
            if modelos:
                print("‚úÖ Ollama disponible con GPU autom√°tica")
                return "ollama", modelos[0]['name']
    except:
        pass
    
    # Verificar llama-cpp-python
    try:
        from llama_cpp import Llama
        print("‚úÖ llama-cpp-python disponible")
        return "gguf", None
    except:
        pass
    
    print("‚ö†Ô∏è No hay backend disponible")
    return None, None


if __name__ == "__main__":
    # Test
    print("üß™ Test de GeneradorUnificado\n")
    
    backend, modelo = detectar_backend_disponible()
    
    if backend == "ollama":
        print(f"\nüì¶ Usando Ollama con {modelo}")
        generador = GeneradorUnificado(usar_ollama=True, modelo_ollama=modelo)
    else:
        print("\nüì¶ Usando llama-cpp-python")
        generador = GeneradorUnificado(usar_ollama=False, modelo_path_gguf="modelos/tu_modelo.gguf")
    
    contenido = """
    Python es un lenguaje de programaci√≥n interpretado de alto nivel.
    Fue creado por Guido van Rossum en 1991.
    """
    
    preguntas = generador.generar_examen(
        contenido,
        {'multiple': 2, 'verdadero_falso': 1}
    )
    
    print(f"\nüìù Preguntas generadas: {len(preguntas)}")
