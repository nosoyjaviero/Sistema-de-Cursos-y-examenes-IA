"""
Sistema de generaci√≥n y evaluaci√≥n de ex√°menes con IA local
"""
from pathlib import Path
from typing import List, Dict, Optional
import json
import sys
from datetime import datetime


class PreguntaExamen:
    """Representa una pregunta de examen"""
    
    def __init__(self, tipo: str, pregunta: str, opciones: List[str] = None, 
                 respuesta_correcta: str = "", puntos: int = 1):
        self.tipo = tipo  # 'multiple', 'combo', 'corta', 'desarrollo'
        self.pregunta = pregunta
        self.opciones = opciones or []
        self.respuesta_correcta = respuesta_correcta
        self.puntos = puntos
        self.respuesta_usuario = None
        self.puntos_obtenidos = 0
    
    def to_dict(self):
        return {
            'tipo': self.tipo,
            'pregunta': self.pregunta,
            'opciones': self.opciones,
            'respuesta_correcta': self.respuesta_correcta,
            'puntos': self.puntos
        }
    
    @classmethod
    def from_dict(cls, data):
        return cls(
            tipo=data['tipo'],
            pregunta=data['pregunta'],
            opciones=data.get('opciones', []),
            respuesta_correcta=data.get('respuesta_correcta', ''),
            puntos=data.get('puntos', 1)
        )


class GeneradorExamenes:
    """Genera ex√°menes usando un modelo LLM local"""
    
    def __init__(self, modelo_path: Optional[str] = None):
        self.modelo_path = modelo_path
        self.llm = None
        if modelo_path:
            self._cargar_modelo()
    
    def _cargar_modelo(self):
        """Carga el modelo LLM"""
        try:
            from llama_cpp import Llama
            print(f"Cargando modelo desde: {self.modelo_path}")
            self.llm = Llama(
                model_path=self.modelo_path,
                n_ctx=4096,  # Contexto largo para documentos grandes
                n_threads=4,
                verbose=False
            )
            print("Modelo cargado exitosamente")
        except Exception as e:
            print(f"Error al cargar modelo: {e}")
            self.llm = None
    
    def generar_prompt_preguntas(self, contenido: str, num_preguntas: Dict[str, int]) -> str:
        """Genera el prompt para crear preguntas"""
        # Limitar contenido pero mantener suficiente contexto
        contenido_limitado = contenido[:6000] if len(contenido) > 6000 else contenido
        
        prompt = f"""Eres un profesor universitario experto creando un examen PR√ÅCTICO y √öTIL para que los estudiantes realmente comprendan la materia.

CONTENIDO DEL MATERIAL DE ESTUDIO:
{contenido_limitado}

OBJETIVO: Crear preguntas que eval√∫en COMPRENSI√ìN PROFUNDA, APLICACI√ìN PR√ÅCTICA y PENSAMIENTO CR√çTICO, NO solo memorizaci√≥n.

INSTRUCCIONES ESTRICTAS:
Genera EXACTAMENTE {sum(num_preguntas.values())} preguntas siguiendo esta distribuci√≥n:
- {num_preguntas.get('multiple', 0)} preguntas de opci√≥n m√∫ltiple
- {num_preguntas.get('corta', 0)} preguntas de respuesta corta
- {num_preguntas.get('desarrollo', 0)} preguntas de desarrollo

CRITERIOS DE CALIDAD OBLIGATORIOS:

1. PREGUNTAS DE OPCI√ìN M√öLTIPLE (tipo: "multiple"):
   - Deben evaluar COMPRENSI√ìN, no solo memoria
   - Incluir casos pr√°cticos o escenarios reales
   - Opciones incorrectas deben ser plausibles pero claramente err√≥neas
   - Evitar preguntas triviales tipo "¬øQu√© es...?"
   - Formato de respuesta: Solo la letra (A, B, C o D)
   - Valor: 3 puntos cada una

2. PREGUNTAS DE RESPUESTA CORTA (tipo: "corta"):
   - Pedir EXPLICACIONES de conceptos clave
   - Solicitar COMPARACIONES entre ideas
   - Preguntar C√ìMO aplicar el conocimiento
   - Requieren 2-4 oraciones de respuesta
   - La respuesta_correcta debe ser una gu√≠a detallada de lo que se espera
   - Valor: 4 puntos cada una

3. PREGUNTAS DE DESARROLLO (tipo: "desarrollo"):
   - Requieren AN√ÅLISIS PROFUNDO y ARGUMENTACI√ìN
   - Deben conectar m√∫ltiples conceptos del material
   - Pedir ejemplos, aplicaciones o cr√≠ticas fundamentadas
   - La respuesta_correcta debe listar criterios de evaluaci√≥n espec√≠ficos
   - Valor: 6 puntos cada una

EJEMPLOS DE BUENAS PREGUNTAS:

Opci√≥n m√∫ltiple BUENA:
"En un proyecto donde necesitas implementar [concepto del material], ¬øcu√°l ser√≠a el enfoque m√°s adecuado considerando las limitaciones mencionadas en el documento?"

Respuesta corta BUENA:
"Explica con tus propias palabras c√≥mo el concepto X se relaciona con Y, y proporciona un ejemplo pr√°ctico de su aplicaci√≥n."

Desarrollo BUENA:
"Analiza cr√≠ticamente la soluci√≥n propuesta en el material. ¬øQu√© ventajas y desventajas presenta? ¬øC√≥mo la mejorar√≠as en un contexto real?"

FORMATO DE RESPUESTA (JSON ESTRICTO - sin comentarios):
{{
  "preguntas": [
    {{
      "tipo": "multiple",
      "pregunta": "[Pregunta pr√°ctica sobre aplicaci√≥n del concepto]",
      "opciones": ["A) [Opci√≥n plausible pero incorrecta]", "B) [Respuesta correcta bien justificada]", "C) [Error conceptual com√∫n]", "D) [Otro error plausible]"],
      "respuesta_correcta": "B",
      "puntos": 3
    }},
    {{
      "tipo": "corta",
      "pregunta": "[Pregunta que requiere explicaci√≥n clara]",
      "respuesta_correcta": "Debe explicar: [punto 1], mencionar [punto 2], y ejemplificar con [punto 3]",
      "puntos": 4
    }},
    {{
      "tipo": "desarrollo",
      "pregunta": "[Pregunta que requiere an√°lisis profundo]",
      "respuesta_correcta": "Criterios: 1) Identifica los conceptos clave [espec√≠ficos], 2) Analiza la relaci√≥n entre ellos, 3) Proporciona ejemplos concretos, 4) Argumenta conclusiones l√≥gicas",
      "puntos": 6
    }}
  ]
}}

IMPORTANTE: 
- Responde SOLO con el JSON v√°lido
- NO agregues texto antes o despu√©s del JSON
- Aseg√∫rate que las preguntas cubran TODO el contenido importante
- Las preguntas deben ser DESAFIANTES pero JUSTAS
- Enf√≥cate en comprensi√≥n y aplicaci√≥n, NO en memorizaci√≥n

JSON:"""
        return prompt
    
    def generar_examen(self, contenido_documento: str, 
                      num_preguntas: Dict[str, int] = None) -> List[PreguntaExamen]:
        """Genera un examen basado en el contenido"""
        if not self.llm:
            print("‚ö†Ô∏è Modelo no cargado. Generando examen de ejemplo...")
            return self._generar_examen_ejemplo()
        
        if num_preguntas is None:
            num_preguntas = {
                'multiple': 8,
                'corta': 5,
                'desarrollo': 3
            }
        
        prompt = self.generar_prompt_preguntas(contenido_documento, num_preguntas)
        
        print(f"ü§ñ Generando {sum(num_preguntas.values())} preguntas con IA...")
        print(f"   üìù {num_preguntas.get('multiple', 0)} opci√≥n m√∫ltiple")
        print(f"   ‚úçÔ∏è {num_preguntas.get('corta', 0)} respuesta corta")
        print(f"   üìñ {num_preguntas.get('desarrollo', 0)} desarrollo")
        
        try:
            respuesta = self.llm(
                prompt,
                max_tokens=3500,  # Aumentado para permitir m√°s preguntas
                temperature=0.8,   # Mayor creatividad para preguntas variadas
                top_p=0.95,
                repeat_penalty=1.2,  # Evitar repetici√≥n
                stop=["```", "\n\n\n"]
            )
            
            texto_respuesta = respuesta['choices'][0]['text'].strip()
            
            # Limpiar posible texto antes/despu√©s del JSON
            if '{' in texto_respuesta:
                inicio = texto_respuesta.find('{')
                fin = texto_respuesta.rfind('}') + 1
                texto_respuesta = texto_respuesta[inicio:fin]
            
            # Intentar parsear JSON
            try:
                datos = json.loads(texto_respuesta)
                preguntas = [PreguntaExamen.from_dict(p) for p in datos['preguntas']]
                
                if len(preguntas) < sum(num_preguntas.values()) * 0.7:
                    print(f"‚ö†Ô∏è Solo se generaron {len(preguntas)} preguntas, esperadas {sum(num_preguntas.values())}")
                else:
                    print(f"‚úÖ Generadas {len(preguntas)} preguntas exitosamente")
                
                return preguntas if preguntas else self._generar_examen_ejemplo()
                
            except json.JSONDecodeError as e:
                print(f"‚ùå Error al parsear JSON de IA: {e}")
                print(f"Respuesta recibida: {texto_respuesta[:200]}...")
                return self._generar_examen_ejemplo()
                
        except Exception as e:
            print(f"‚ùå Error al generar examen con IA: {e}")
            return self._generar_examen_ejemplo()
    
    def _generar_examen_ejemplo(self) -> List[PreguntaExamen]:
        """Genera un examen de ejemplo sin IA"""
        print("üìù Generando examen de ejemplo (sin modelo IA cargado)")
        return [
            # Preguntas de opci√≥n m√∫ltiple
            PreguntaExamen(
                tipo='multiple',
                pregunta='¬øCu√°l de las siguientes afirmaciones describe mejor la idea central del documento?',
                opciones=[
                    'A) Presenta una lista de datos sin conexi√≥n',
                    'B) Desarrolla conceptos fundamentales con ejemplos pr√°cticos',
                    'C) Solo contiene definiciones t√©cnicas',
                    'D) Es √∫nicamente material de referencia'
                ],
                respuesta_correcta='B',
                puntos=3
            ),
            PreguntaExamen(
                tipo='multiple',
                pregunta='Si tuvieras que aplicar los conceptos del documento en un proyecto real, ¬øqu√© factor ser√≠a m√°s cr√≠tico considerar?',
                opciones=[
                    'A) El costo de implementaci√≥n',
                    'B) La comprensi√≥n profunda de los fundamentos te√≥ricos',
                    'C) La velocidad de ejecuci√≥n',
                    'D) La popularidad de la tecnolog√≠a'
                ],
                respuesta_correcta='B',
                puntos=3
            ),
            PreguntaExamen(
                tipo='multiple',
                pregunta='¬øQu√© relaci√≥n existe entre los principales conceptos presentados en el material?',
                opciones=[
                    'A) Son independientes y no se relacionan',
                    'B) Cada concepto contradice al anterior',
                    'C) Se complementan formando un marco conceptual integrado',
                    'D) Solo uno de ellos es relevante'
                ],
                respuesta_correcta='C',
                puntos=3
            ),
            PreguntaExamen(
                tipo='multiple',
                pregunta='Al evaluar la aplicabilidad del contenido, ¬øcu√°l ser√≠a la mejor estrategia?',
                opciones=[
                    'A) Memorizar todas las definiciones',
                    'B) Comprender los principios y adaptarlos al contexto',
                    'C) Copiar los ejemplos tal cual',
                    'D) Ignorar la teor√≠a y enfocarse en la pr√°ctica'
                ],
                respuesta_correcta='B',
                puntos=3
            ),
            # Preguntas de respuesta corta
            PreguntaExamen(
                tipo='corta',
                pregunta='Explica con tus propias palabras los 3 conceptos m√°s importantes del material y c√≥mo se relacionan entre s√≠.',
                respuesta_correcta='Debe identificar 3 conceptos clave del material, explicar cada uno brevemente, y mostrar c√≥mo se conectan o complementan. Se espera comprensi√≥n conceptual, no simple repetici√≥n.',
                puntos=4
            ),
            PreguntaExamen(
                tipo='corta',
                pregunta='Describe una situaci√≥n real donde podr√≠as aplicar el conocimiento adquirido y explica c√≥mo lo har√≠as.',
                respuesta_correcta='Debe proporcionar un ejemplo concreto y pr√°ctico, explicar el contexto de aplicaci√≥n, y detallar los pasos o consideraciones necesarias para implementarlo.',
                puntos=4
            ),
            PreguntaExamen(
                tipo='corta',
                pregunta='¬øQu√© diferencia existe entre los dos enfoques principales discutidos en el material? Proporciona un ejemplo de cada uno.',
                respuesta_correcta='Debe identificar los enfoques principales, explicar sus diferencias fundamentales, y dar ejemplos espec√≠ficos que ilustren cada enfoque.',
                puntos=4
            ),
            # Preguntas de desarrollo
            PreguntaExamen(
                tipo='desarrollo',
                pregunta='Analiza cr√≠ticamente el material: identifica sus fortalezas, posibles limitaciones, y prop√≥n c√≥mo podr√≠as extender o mejorar los conceptos presentados bas√°ndote en tu comprensi√≥n.',
                respuesta_correcta='Criterios: 1) Identifica al menos 2 fortalezas espec√≠ficas del material con justificaci√≥n, 2) Reconoce limitaciones o √°reas de mejora, 3) Propone extensiones o mejoras fundamentadas, 4) Demuestra pensamiento cr√≠tico y comprensi√≥n profunda.',
                puntos=6
            ),
            PreguntaExamen(
                tipo='desarrollo',
                pregunta='Integra los conceptos principales del documento en un marco coherente. Explica c√≥mo cada elemento contribuye al todo y qu√© implicaciones pr√°cticas tiene esta integraci√≥n.',
                respuesta_correcta='Criterios: 1) Identifica los conceptos principales, 2) Explica las relaciones e interdependencias, 3) Construye un marco integrado l√≥gico, 4) Discute implicaciones pr√°cticas espec√≠ficas.',
                puntos=6
            ),
        ]
    
    def evaluar_respuesta(self, pregunta: PreguntaExamen, respuesta_usuario: str) -> tuple[int, str]:
        """Eval√∫a una respuesta del usuario"""
        if pregunta.tipo == 'multiple':
            return self._evaluar_multiple(pregunta, respuesta_usuario)
        elif pregunta.tipo == 'corta':
            return self._evaluar_corta(pregunta, respuesta_usuario)
        elif pregunta.tipo == 'desarrollo':
            return self._evaluar_desarrollo(pregunta, respuesta_usuario)
        else:
            return 0, "Tipo de pregunta no soportado"
    
    def _evaluar_multiple(self, pregunta: PreguntaExamen, respuesta: str) -> tuple[int, str]:
        """Eval√∫a pregunta de opci√≥n m√∫ltiple"""
        respuesta = respuesta.strip().upper()
        correcta = pregunta.respuesta_correcta.strip().upper()
        
        if respuesta == correcta or respuesta == correcta[0]:
            return pregunta.puntos, "¬°Correcto!"
        else:
            return 0, f"Incorrecto. La respuesta correcta es: {pregunta.respuesta_correcta}"
    
    def _evaluar_corta(self, pregunta: PreguntaExamen, respuesta: str) -> tuple[int, str]:
        """Eval√∫a pregunta de respuesta corta con IA"""
        if not respuesta or len(respuesta.strip()) < 10:
            return 0, "‚ùå Respuesta insuficiente o vac√≠a. Se requiere una explicaci√≥n clara y completa."
        
        if not self.llm:
            # Sin IA, evaluaci√≥n b√°sica por longitud y palabras clave
            palabras = len(respuesta.split())
            if palabras < 15:
                return pregunta.puntos // 4, "‚ö†Ô∏è Respuesta muy breve. Se esperaba mayor desarrollo."
            elif palabras < 30:
                return pregunta.puntos // 2, "‚ö†Ô∏è Respuesta aceptable pero podr√≠a ser m√°s detallada."
            else:
                return int(pregunta.puntos * 0.7), "‚úì Respuesta con buen desarrollo (evaluaci√≥n autom√°tica)."
        
        prompt = f"""Eres un profesor ESTRICTO evaluando una respuesta corta. S√© cr√≠tico pero justo.

PREGUNTA: {pregunta.pregunta}

CRITERIOS DE EVALUACI√ìN: {pregunta.respuesta_correcta}

RESPUESTA DEL ESTUDIANTE: 
{respuesta}

PUNTOS M√ÅXIMOS: {pregunta.puntos}

INSTRUCCIONES DE EVALUACI√ìN ESTRICTA:
1. ¬øLa respuesta demuestra COMPRENSI√ìN REAL del concepto? (no solo copiar)
2. ¬øIncluye los elementos clave mencionados en los criterios?
3. ¬øProporciona ejemplos o explicaciones claras?
4. ¬øLa redacci√≥n es coherente y precisa?

ESCALA:
- {pregunta.puntos} puntos: Excelente, completa todos los criterios con claridad
- {int(pregunta.puntos * 0.75)}-{pregunta.puntos - 1} puntos: Buena, cumple la mayor√≠a de criterios
- {int(pregunta.puntos * 0.5)}-{int(pregunta.puntos * 0.7)} puntos: Aceptable, cumple criterios b√°sicos pero falta profundidad
- {int(pregunta.puntos * 0.25)}-{int(pregunta.puntos * 0.45)} puntos: Insuficiente, solo aspectos superficiales
- 0-{int(pregunta.puntos * 0.2)} puntos: Inadecuada, no demuestra comprensi√≥n

Responde SOLO con JSON:
{{
  "puntos": <n√∫mero de 0 a {pregunta.puntos}>,
  "feedback": "Feedback espec√≠fico: qu√© est√° bien, qu√© falta, c√≥mo mejorar"
}}"""
        
        try:
            resultado = self.llm(prompt, max_tokens=250, temperature=0.2)
            texto = resultado['choices'][0]['text'].strip()
            
            if '{' in texto:
                inicio = texto.find('{')
                fin = texto.rfind('}') + 1
                texto = texto[inicio:fin]
            
            datos = json.loads(texto)
            puntos = min(datos['puntos'], pregunta.puntos)
            return puntos, datos['feedback']
        except:
            # Fallback con evaluaci√≥n por similitud de longitud
            palabras = len(respuesta.split())
            if palabras < 20:
                return pregunta.puntos // 3, "‚ö†Ô∏è Respuesta incompleta. Falta desarrollo y profundidad."
            else:
                return int(pregunta.puntos * 0.6), "‚úì Respuesta aceptable (evaluaci√≥n autom√°tica limitada)."
    
    def _evaluar_desarrollo(self, pregunta: PreguntaExamen, respuesta: str) -> tuple[int, str]:
        """Eval√∫a pregunta de desarrollo con IA"""
        if not respuesta or len(respuesta.strip()) < 50:
            return 0, "‚ùå Respuesta insuficiente. Las preguntas de desarrollo requieren an√°lisis profundo y extenso."
        
        if not self.llm:
            # Sin IA, evaluaci√≥n b√°sica por longitud y estructura
            palabras = len(respuesta.split())
            if palabras < 50:
                return pregunta.puntos // 4, "‚ö†Ô∏è Respuesta muy breve para una pregunta de desarrollo."
            elif palabras < 100:
                return pregunta.puntos // 2, "‚ö†Ô∏è Respuesta insuficiente. Se requiere mayor profundidad."
            else:
                return int(pregunta.puntos * 0.7), "‚úì Respuesta con desarrollo aceptable (evaluaci√≥n autom√°tica)."
        
        prompt = f"""Eres un profesor universitario EXIGENTE evaluando una pregunta de desarrollo. S√© CR√çTICO pero JUSTO.

PREGUNTA: {pregunta.pregunta}

CRITERIOS DE EVALUACI√ìN ESPEC√çFICOS:
{pregunta.respuesta_correcta}

RESPUESTA DEL ESTUDIANTE:
{respuesta}

PUNTOS M√ÅXIMOS: {pregunta.puntos}

EVALUACI√ìN DETALLADA - Analiza estos 5 aspectos:

1. COMPRENSI√ìN CONCEPTUAL ({int(pregunta.puntos * 0.25)} pts m√°x)
   - ¬øDemuestra entendimiento profundo de los conceptos?
   - ¬øIdentifica correctamente los elementos clave?

2. AN√ÅLISIS Y ARGUMENTACI√ìN ({int(pregunta.puntos * 0.25)} pts m√°x)
   - ¬øPresenta argumentos l√≥gicos y bien fundamentados?
   - ¬øConecta ideas de manera coherente?

3. PROFUNDIDAD Y EXTENSI√ìN ({int(pregunta.puntos * 0.2)} pts m√°x)
   - ¬øExplora el tema con suficiente profundidad?
   - ¬øProporciona ejemplos o casos espec√≠ficos?

4. PENSAMIENTO CR√çTICO ({int(pregunta.puntos * 0.2)} pts m√°x)
   - ¬øAnaliza cr√≠ticamente en lugar de solo describir?
   - ¬øPropone ideas originales o mejoras?

5. CLARIDAD Y ESTRUCTURA ({int(pregunta.puntos * 0.1)} pts m√°x)
   - ¬øLa respuesta est√° bien organizada?
   - ¬øSe expresa con claridad?

IMPORTANTE: 
- Penaliza severamente respuestas superficiales o gen√©ricas
- Recompensa an√°lisis profundo y pensamiento cr√≠tico
- El m√°ximo solo se otorga a respuestas excepcionales

Responde SOLO con JSON:
{{
  "puntos": <n√∫mero de 0 a {pregunta.puntos}>,
  "feedback": "Feedback detallado: (1) Lo que est√° bien hecho, (2) Lo que falta o necesita mejora, (3) Sugerencias espec√≠ficas"
}}"""
        
        try:
            resultado = self.llm(prompt, max_tokens=600, temperature=0.2)
            texto = resultado['choices'][0]['text'].strip()
            
            if '{' in texto:
                inicio = texto.find('{')
                fin = texto.rfind('}') + 1
                texto = texto[inicio:fin]
            
            datos = json.loads(texto)
            puntos = min(datos['puntos'], pregunta.puntos)
            return puntos, datos['feedback']
        except Exception as e:
            print(f"Error en evaluaci√≥n de desarrollo: {e}")
            # Fallback con evaluaci√≥n por longitud
            palabras = len(respuesta.split())
            if palabras < 80:
                return int(pregunta.puntos * 0.4), "‚ö†Ô∏è Respuesta breve. Se espera mayor desarrollo y profundidad."
            else:
                return int(pregunta.puntos * 0.65), "‚úì Respuesta con desarrollo (evaluaci√≥n autom√°tica limitada)."


def guardar_examen(preguntas: List[PreguntaExamen], ruta: Path):
    """Guarda el examen en formato JSON"""
    datos = {
        'fecha_creacion': datetime.now().isoformat(),
        'preguntas': [p.to_dict() for p in preguntas]
    }
    with open(ruta, 'w', encoding='utf-8') as f:
        json.dump(datos, f, ensure_ascii=False, indent=2)


def cargar_examen(ruta: Path) -> List[PreguntaExamen]:
    """Carga un examen desde JSON"""
    with open(ruta, 'r', encoding='utf-8') as f:
        datos = json.load(f)
    return [PreguntaExamen.from_dict(p) for p in datos['preguntas']]


if __name__ == "__main__":
    print("M√≥dulo de generaci√≥n de ex√°menes")
    print("Usar desde examinator_interactivo.py")
