# 游 Gu칤a de Uso: llama.cpp binarios precompilados

## Paso 1: Extraer archivo

1. Extrae `llama-b7084-bin-win-cuda-12.4-x64.zip` en una carpeta
2. Por ejemplo: `C:\llama-cpp\`

## Paso 2: Probar GPU

Abre PowerShell en la carpeta extra칤da y ejecuta:

```powershell
cd C:\llama-cpp\  # o donde lo hayas extra칤do

# Probar que funciona
.\llama-cli.exe --version

# Ejecutar servidor con GPU
.\llama-server.exe `
  --model "C:\Users\Fela\Documents\Proyectos\Examinator\modelos\qwen2.5-3b-instruct-q4_k_m.gguf" `
  --n-gpu-layers 30 `
  --port 8080 `
  --host 0.0.0.0 `
  --ctx-size 8192

# En otra terminal, ver uso de GPU:
nvidia-smi
```

Deber칤as ver en nvidia-smi que `llama-server.exe` est치 usando la GPU.

## Paso 3: Probar desde tu c칩digo Python

El servidor estar치 disponible en `http://localhost:8080`

Puedes hacer peticiones HTTP:

```python
import requests

response = requests.post(
    "http://localhost:8080/completion",
    json={
        "prompt": "쯈u칠 es Python?",
        "n_predict": 100,
        "temperature": 0.7
    }
)

print(response.json()["content"])
```

## Paso 4: Integrar con tu aplicaci칩n

Opci칩n A: Crear un adaptador que tu c칩digo use el servidor llama.cpp
Opci칩n B: Iniciar llama-server autom치ticamente desde Python
Opci칩n C: Usar como servicio Windows que inicia con el sistema

쯈uieres que cree el adaptador para tu c칩digo actual?
