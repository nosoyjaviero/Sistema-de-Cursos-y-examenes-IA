# üéâ ¬°GPU Configurada Exitosamente!

## ‚úÖ Estado Actual

**Backend activo:** Ollama v0.12.11
**Modelo instalado:** llama3.2:3b (2.0 GB)
**GPU:** 100% activa (detectada autom√°ticamente)

## üìÅ Archivos Creados

### Scripts de Generaci√≥n
- ‚úÖ `generador_examenes_ollama.py` - Generador solo con Ollama
- ‚úÖ `generador_unificado.py` - Detecta autom√°ticamente Ollama o llama-cpp-python
- ‚úÖ `comparar_rendimiento.py` - Compara velocidad GPU vs CPU

### Scripts de Inicio
- ‚úÖ `iniciar_ollama.ps1` - Inicia sistema con Ollama

### Documentaci√≥n
- ‚úÖ `GUIA_OLLAMA.md` - Gu√≠a completa de Ollama
- ‚úÖ `INSTALAR_LLAMA_GPU.md` - Gu√≠a alternativa para llama-cpp-python

## üöÄ C√≥mo Usar

### Opci√≥n 1: Script de Test (m√°s simple)

```powershell
python generador_examenes_ollama.py
```

### Opci√≥n 2: Generador Unificado (recomendado)

```powershell
python generador_unificado.py
```

### Opci√≥n 3: Comparar Rendimiento

```powershell
python comparar_rendimiento.py
```

### Opci√≥n 4: Script de Inicio PowerShell

```powershell
.\iniciar_ollama.ps1
```

## üìä Verificar que usa GPU

En otra terminal, ejecuta:

```powershell
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" ps
```

Deber√≠as ver:
```
NAME           ID         SIZE    PROCESSOR    CONTEXT
llama3.2:3b    a80c...    2.8 GB  100% GPU     4096
```

Si dice **"100% GPU"** = ¬°Todo correcto! üéâ

## üéØ Pr√≥ximos Pasos

### 1. Integrar en tu API Web

Para usar Ollama en `api_server.py`, tendr√°s que modificarlo para usar `GeneradorUnificado`:

```python
from generador_unificado import GeneradorUnificado

# En lugar de:
generador_actual = GeneradorDosPasos(modelo_path=modelo_path, n_gpu_layers=gpu_layers)

# Usar:
generador_actual = GeneradorUnificado(usar_ollama=True, modelo_ollama="llama3.2:3b")
```

### 2. Descargar M√°s Modelos

```powershell
# Modelo peque√±o y r√°pido (1.3 GB)
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" pull llama3.2:1b

# Modelo m√°s preciso (4.7 GB)
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" pull llama3.1:8b

# Alternativa en espa√±ol
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" pull qwen2.5:3b
```

### 3. Ver Modelos Instalados

```powershell
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" list
```

## üÜö Comparaci√≥n: Ollama vs llama-cpp-python

| Caracter√≠stica | Ollama | llama-cpp-python |
|----------------|---------|------------------|
| **GPU** | ‚úÖ Autom√°tica | ‚ö†Ô∏è Manual (n_gpu_layers) |
| **Instalaci√≥n** | ‚úÖ 1 click | ‚ùå Compilar con CUDA |
| **Gesti√≥n modelos** | ‚úÖ `ollama pull` | ‚ö†Ô∏è Descargar .gguf manual |
| **Facilidad** | ‚úÖ‚úÖ‚úÖ Muy f√°cil | ‚ö†Ô∏è Media-Dif√≠cil |
| **Rendimiento** | ‚úÖ Excelente | ‚úÖ Bueno |
| **Integraci√≥n Python** | ‚úÖ Requests HTTP | ‚úÖ Nativa |

## üí° Consejos

1. **Mant√©n Ollama corriendo en segundo plano** - Se inicia autom√°ticamente al arrancar Windows
2. **Verifica GPU peri√≥dicamente** con `ollama ps`
3. **Si Ollama no responde**, reinicia el servicio:
   ```powershell
   Get-Process ollama | Stop-Process -Force
   & "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" serve
   ```

## üìö Recursos

- Documentaci√≥n Ollama: https://github.com/ollama/ollama
- Modelos disponibles: https://ollama.com/library
- Discord Ollama: https://discord.gg/ollama

## üéì Ejemplo de Uso Completo

```python
from generador_unificado import GeneradorUnificado

# Crear generador (usa GPU autom√°ticamente)
generador = GeneradorUnificado(
    usar_ollama=True,
    modelo_ollama="llama3.2:3b"
)

# Contenido del curso
contenido = """
Tu contenido educativo aqu√≠...
"""

# Generar examen
preguntas = generador.generar_examen(
    contenido_documento=contenido,
    num_preguntas={
        'multiple': 5,
        'verdadero_falso': 3,
        'corta': 2
    },
    ajustes_modelo={
        'temperature': 0.25,
        'max_tokens': 3000
    }
)

# Ver resultados
for i, p in enumerate(preguntas, 1):
    print(f"{i}. [{p.tipo}] {p.pregunta}")
    if p.opciones:
        for op in p.opciones:
            print(f"   {op}")
    print(f"   ‚úì Respuesta: {p.respuesta_correcta}")
```

## ‚ú® ¬°Listo para Producci√≥n!

Ya tienes todo configurado para usar GPU en tus modelos. El sistema:

- ‚úÖ Detecta GPU autom√°ticamente
- ‚úÖ Funciona con Ollama (recomendado)
- ‚úÖ Fallback a llama-cpp-python si es necesario
- ‚úÖ Scripts de prueba incluidos
- ‚úÖ Documentaci√≥n completa

**¬°A generar ex√°menes a toda velocidad! üöÄ**
