"""
Script para verificar si la GPU est√° disponible y configurada correctamente
"""

print("="*60)
print("üîç DIAGN√ìSTICO DE GPU PARA LLAMA-CPP-PYTHON")
print("="*60)

# 1. Verificar llama-cpp-python
print("\n1Ô∏è‚É£ Verificando llama-cpp-python...")
try:
    import llama_cpp
    print(f"   ‚úÖ llama-cpp-python instalado")
    print(f"   üì¶ Versi√≥n: {llama_cpp.__version__}")
    
    # Verificar si fue compilado con CUDA
    try:
        # Intentar acceder a funciones CUDA
        from llama_cpp import llama_cpp
        print(f"   ‚ÑπÔ∏è  M√≥dulo llama_cpp importado correctamente")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  No se pudo importar llama_cpp: {e}")
        
except ImportError:
    print(f"   ‚ùå llama-cpp-python NO est√° instalado")
    print(f"   üí° Instalar con: pip install llama-cpp-python")

# 2. Verificar PyTorch y CUDA
print("\n2Ô∏è‚É£ Verificando PyTorch y CUDA...")
try:
    import torch
    print(f"   ‚úÖ PyTorch instalado")
    print(f"   üì¶ Versi√≥n: {torch.__version__}")
    
    if torch.cuda.is_available():
        print(f"   ‚úÖ CUDA disponible")
        print(f"   üéÆ GPU: {torch.cuda.get_device_name(0)}")
        print(f"   üìä VRAM Total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        print(f"   üî¢ Compute Capability: {torch.cuda.get_device_capability(0)}")
        print(f"   üìç CUDA Version: {torch.version.cuda}")
    else:
        print(f"   ‚ö†Ô∏è  CUDA NO disponible en PyTorch")
        print(f"   üí° Puede que PyTorch est√© instalado sin CUDA")
        
except ImportError:
    print(f"   ‚ÑπÔ∏è  PyTorch no est√° instalado (no es obligatorio)")

# 3. Verificar variables de entorno
print("\n3Ô∏è‚É£ Verificando variables de entorno...")
import os

cuda_vars = ['CUDA_PATH', 'CUDA_HOME', 'CUDA_VISIBLE_DEVICES']
for var in cuda_vars:
    valor = os.environ.get(var)
    if valor:
        print(f"   ‚úÖ {var}: {valor}")
    else:
        print(f"   ‚ö†Ô∏è  {var} no est√° configurada")

# 4. Intentar cargar un modelo con GPU
print("\n4Ô∏è‚É£ Probando carga de modelo con GPU...")
try:
    from llama_cpp import Llama
    from pathlib import Path
    
    # Buscar un modelo
    modelo_path = None
    modelos_dir = Path("modelos")
    if modelos_dir.exists():
        modelos = list(modelos_dir.glob("*.gguf"))
        if modelos:
            modelo_path = str(modelos[0])
            print(f"   üìÅ Modelo encontrado: {modelo_path}")
            
            print(f"   üîÑ Intentando cargar con n_gpu_layers=1...")
            try:
                llm = Llama(
                    model_path=modelo_path,
                    n_ctx=512,
                    n_gpu_layers=1,
                    verbose=True
                )
                print(f"   ‚úÖ Modelo cargado con GPU")
                
                # Hacer una inferencia de prueba
                print(f"   üß™ Probando inferencia...")
                respuesta = llm("Hola", max_tokens=5)
                print(f"   ‚úÖ Inferencia exitosa")
                
                del llm  # Liberar memoria
                
            except Exception as e:
                print(f"   ‚ùå Error cargando con GPU: {e}")
                print(f"   üí° Probablemente llama-cpp-python no fue compilado con CUDA")
        else:
            print(f"   ‚ö†Ô∏è  No se encontraron modelos .gguf en {modelos_dir}")
    else:
        print(f"   ‚ö†Ô∏è  Carpeta 'modelos' no existe")
        
except Exception as e:
    print(f"   ‚ùå Error: {e}")

# 5. Recomendaciones
print("\n" + "="*60)
print("üìã RECOMENDACIONES")
print("="*60)

print("""
Para usar GPU con llama-cpp-python necesitas:

1. Tener una GPU NVIDIA con CUDA instalado
2. Instalar llama-cpp-python compilado con CUDA:
   
   OPCI√ìN A (precompilado con CUDA):
   pip uninstall llama-cpp-python -y
   pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121
   
   OPCI√ìN B (compilar desde c√≥digo):
   $env:CMAKE_ARGS="-DLLAMA_CUBLAS=on"
   pip install llama-cpp-python --force-reinstall --no-cache-dir

3. Verificar que CUDA funciona:
   nvidia-smi

Si ves tu GPU en nvidia-smi pero llama-cpp no la usa,
probablemente necesitas reinstalar llama-cpp-python con soporte CUDA.
""")

print("="*60)
